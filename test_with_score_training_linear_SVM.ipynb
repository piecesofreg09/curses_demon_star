{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle, os, math\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90029, 221)\n",
      "(81026, 221)\n",
      "class 0 has 65183 points\n",
      "class 1 has 15843 points\n",
      "15843 + 15843 = 31686\n",
      "Wall time: 871 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_dir = os.path.join(os.curdir, 'Data', 'Score', '400000_68per', 'data_basic.pkl')\n",
    "with open(data_dir, 'rb') as in_file:\n",
    "    ot = pickle.load(in_file)\n",
    "data_pics = ot['data']\n",
    "target_pics = ot['target']\n",
    "print(data_pics.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_pics, target_pics, test_size=0.1, random_state=randint(100, 10000))\n",
    "print(X_train.shape)\n",
    "X_train_train, X_vali, y_train_train, y_vali = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=randint(100, 10000))\n",
    "print('class 0 has ' + str(len(y_train.index[y_train[0] == 0].tolist())) + ' points')\n",
    "print('class 1 has ' + str(len(y_train.index[y_train[0] == 1].tolist())) + ' points')\n",
    "\n",
    "index_0 = y_train.index[y_train[0] == 0].tolist()\n",
    "index_1 = y_train.index[y_train[0] != 0].tolist()\n",
    "index_0_comparable_to_1 = np.random.choice(index_0, math.floor(len(index_1) * 1))\n",
    "samples = np.concatenate([index_1, index_0_comparable_to_1])\n",
    "print(str(len(index_1)) + ' + ' + str(len(index_0_comparable_to_1)) + ' = ' + str(len(samples)))\n",
    "\n",
    "\n",
    "small_data = data_pics.iloc[samples, :]\n",
    "small_target = target_pics.iloc[samples, :]\n",
    "\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(\n",
    "    small_data, small_target, test_size=0.3, random_state=randint(100, 10000))\n",
    "X_train_train_small, X_vali_small, y_train_train_small, y_vali_small = train_test_split(\n",
    "    X_train_small, y_train_small, test_size=0.3, random_state=randint(100, 10000))\n",
    "\n",
    "y_train_small_m = np.ravel(y_train_small)\n",
    "y_test_small_m = np.ravel(y_test_small)\n",
    "y_train_train_small_m = np.ravel(y_train_train_small)\n",
    "y_vali_small_m = np.ravel(y_vali_small)\n",
    "y_train_m = np.ravel(y_train)\n",
    "y_test_m = np.ravel(y_test)\n",
    "y_train_train_m = np.ravel(y_train_train)\n",
    "y_vali_m = np.ravel(y_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.28341634\n",
      "Iteration 2, loss = 0.72230461\n",
      "Iteration 3, loss = 0.73042272\n",
      "Iteration 4, loss = 0.70975215\n",
      "Iteration 5, loss = 0.70465709\n",
      "Iteration 6, loss = 0.73560345\n",
      "Iteration 7, loss = 0.70587998\n",
      "Iteration 8, loss = 0.69941877\n",
      "Iteration 9, loss = 0.69033928\n",
      "Iteration 10, loss = 0.68256500\n",
      "Iteration 11, loss = 0.67955632\n",
      "Iteration 12, loss = 0.67750325\n",
      "Iteration 13, loss = 0.67529472\n",
      "Iteration 14, loss = 0.67317561\n",
      "Iteration 15, loss = 0.67118424\n",
      "Iteration 16, loss = 0.67102048\n",
      "Iteration 17, loss = 0.66935258\n",
      "Iteration 18, loss = 0.66851202\n",
      "Iteration 19, loss = 0.66693017\n",
      "Iteration 20, loss = 0.67192036\n",
      "Iteration 21, loss = 0.66257172\n",
      "Iteration 22, loss = 0.66153741\n",
      "Iteration 23, loss = 0.66050878\n",
      "Iteration 24, loss = 0.65825059\n",
      "Iteration 25, loss = 0.65835582\n",
      "Iteration 26, loss = 0.66097896\n",
      "Iteration 27, loss = 0.65768561\n",
      "Iteration 28, loss = 0.65655742\n",
      "Iteration 29, loss = 0.65419731\n",
      "Iteration 30, loss = 0.65389481\n",
      "Iteration 31, loss = 0.65144654\n",
      "Iteration 32, loss = 0.66778024\n",
      "Iteration 33, loss = 0.66213726\n",
      "Iteration 34, loss = 0.65881370\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6471702082895013"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc_st = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32, 16),\n",
    "                        alpha=0.15, max_iter=1000, batch_size=5000,\n",
    "                        verbose=True, learning_rate_init=0.04, tol=1e-5,\n",
    "                        learning_rate='adaptive')\n",
    "\n",
    "mlpc_st.fit(X_train_small, y_train_small_m)\n",
    "mlpc_st.score(X_test_small, y_test_small_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hlu82\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.495332004615546"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=10.0, verbose=True, max_iter=20000)\n",
    "lsvc.fit(X_train, y_train_m)\n",
    "lsvc.score(X_test_small, y_test_small_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 218.97, NNZs: 204, Bias: -154.643969, T: 22092, Avg. loss: 259.634918\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 155.08, NNZs: 206, Bias: -67.204312, T: 44184, Avg. loss: 53.137236\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 117.47, NNZs: 206, Bias: -46.664996, T: 66276, Avg. loss: 31.033312\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 92.95, NNZs: 206, Bias: -34.049737, T: 88368, Avg. loss: 22.491312\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 78.65, NNZs: 206, Bias: -28.885993, T: 110460, Avg. loss: 17.516258\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 69.42, NNZs: 206, Bias: -22.851776, T: 132552, Avg. loss: 14.516721\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 60.99, NNZs: 206, Bias: -18.403809, T: 154644, Avg. loss: 12.449717\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 54.96, NNZs: 206, Bias: -18.059911, T: 176736, Avg. loss: 10.685782\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 49.14, NNZs: 206, Bias: -16.490752, T: 198828, Avg. loss: 9.704033\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 45.21, NNZs: 206, Bias: -14.742929, T: 220920, Avg. loss: 8.689455\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 41.50, NNZs: 206, Bias: -13.690915, T: 243012, Avg. loss: 7.921465\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 38.40, NNZs: 206, Bias: -12.543156, T: 265104, Avg. loss: 7.230947\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 36.08, NNZs: 206, Bias: -10.809572, T: 287196, Avg. loss: 6.783894\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 34.21, NNZs: 206, Bias: -10.736087, T: 309288, Avg. loss: 6.276284\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 31.56, NNZs: 206, Bias: -9.437019, T: 331380, Avg. loss: 5.949068\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 30.22, NNZs: 206, Bias: -8.287473, T: 353472, Avg. loss: 5.588077\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 28.72, NNZs: 206, Bias: -8.464353, T: 375564, Avg. loss: 5.292788\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 27.32, NNZs: 206, Bias: -7.597456, T: 397656, Avg. loss: 5.050215\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 26.42, NNZs: 206, Bias: -7.825168, T: 419748, Avg. loss: 4.839159\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 25.41, NNZs: 206, Bias: -7.934420, T: 441840, Avg. loss: 4.563157\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 24.38, NNZs: 206, Bias: -7.010708, T: 463932, Avg. loss: 4.438646\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 22.94, NNZs: 206, Bias: -6.939658, T: 486024, Avg. loss: 4.218488\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 22.34, NNZs: 206, Bias: -6.940607, T: 508116, Avg. loss: 4.038056\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 21.86, NNZs: 206, Bias: -6.241829, T: 530208, Avg. loss: 3.928525\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 21.39, NNZs: 206, Bias: -5.876406, T: 552300, Avg. loss: 3.756296\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 20.82, NNZs: 206, Bias: -5.704929, T: 574392, Avg. loss: 3.737148\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 20.46, NNZs: 206, Bias: -5.642798, T: 596484, Avg. loss: 3.573673\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 20.26, NNZs: 206, Bias: -5.765928, T: 618576, Avg. loss: 3.472147\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 19.47, NNZs: 206, Bias: -5.610979, T: 640668, Avg. loss: 3.413315\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 19.00, NNZs: 206, Bias: -5.382257, T: 662760, Avg. loss: 3.248400\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 18.48, NNZs: 206, Bias: -5.451321, T: 684852, Avg. loss: 3.215555\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 18.16, NNZs: 206, Bias: -4.851327, T: 706944, Avg. loss: 3.143261\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 17.67, NNZs: 206, Bias: -5.124350, T: 729036, Avg. loss: 3.035955\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 17.39, NNZs: 206, Bias: -4.879430, T: 751128, Avg. loss: 2.965265\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 16.91, NNZs: 206, Bias: -4.709543, T: 773220, Avg. loss: 2.900084\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 16.27, NNZs: 206, Bias: -4.720322, T: 795312, Avg. loss: 2.883695\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 16.02, NNZs: 206, Bias: -4.375843, T: 817404, Avg. loss: 2.798511\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 15.70, NNZs: 206, Bias: -4.315571, T: 839496, Avg. loss: 2.744710\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 15.43, NNZs: 206, Bias: -4.385183, T: 861588, Avg. loss: 2.701117\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 15.16, NNZs: 206, Bias: -4.018430, T: 883680, Avg. loss: 2.635136\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 14.86, NNZs: 206, Bias: -4.074042, T: 905772, Avg. loss: 2.550233\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 14.60, NNZs: 206, Bias: -3.867933, T: 927864, Avg. loss: 2.547814\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 14.55, NNZs: 206, Bias: -3.816407, T: 949956, Avg. loss: 2.492262\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 14.09, NNZs: 206, Bias: -3.805123, T: 972048, Avg. loss: 2.473570\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 13.78, NNZs: 206, Bias: -3.935921, T: 994140, Avg. loss: 2.395393\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 13.70, NNZs: 206, Bias: -3.715316, T: 1016232, Avg. loss: 2.392167\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 13.57, NNZs: 206, Bias: -3.811179, T: 1038324, Avg. loss: 2.343796\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 13.34, NNZs: 206, Bias: -3.829298, T: 1060416, Avg. loss: 2.318874\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 13.24, NNZs: 206, Bias: -3.911862, T: 1082508, Avg. loss: 2.275595\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 13.14, NNZs: 206, Bias: -3.590871, T: 1104600, Avg. loss: 2.234115\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 13.06, NNZs: 206, Bias: -3.653908, T: 1126692, Avg. loss: 2.237579\n",
      "Total training time: 1.13 seconds.\n",
      "Convergence after 51 epochs took 1.13 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4977820025348543"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdc = SGDClassifier(verbose=True, max_iter=20000, tol=1e-4, penalty='l2', learning_rate='optimal')\n",
    "sgdc.fit(X_train_small, y_train_small_m)\n",
    "sgdc.score(X_test_small, y_test_small_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
