{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle, os, math\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90029, 221)\n",
      "(81026, 221)\n",
      "class 0 has 65219 points\n",
      "class 1 has 15807 points\n",
      "15807 + 15807 = 31614\n",
      "Wall time: 868 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_dir = os.path.join(os.curdir, 'Data', 'Score', '400000_68per', 'data_basic.pkl')\n",
    "with open(data_dir, 'rb') as in_file:\n",
    "    ot = pickle.load(in_file)\n",
    "data_pics = ot['data']\n",
    "target_pics = ot['target']\n",
    "print(data_pics.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_pics, target_pics, test_size=0.1, random_state=randint(100, 10000))\n",
    "print(X_train.shape)\n",
    "X_train_train, X_vali, y_train_train, y_vali = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=randint(100, 10000))\n",
    "print('class 0 has ' + str(len(y_train.index[y_train[0] == 0].tolist())) + ' points')\n",
    "print('class 1 has ' + str(len(y_train.index[y_train[0] == 1].tolist())) + ' points')\n",
    "\n",
    "index_0 = y_train.index[y_train[0] == 0].tolist()\n",
    "index_1 = y_train.index[y_train[0] != 0].tolist()\n",
    "index_0_comparable_to_1 = np.random.choice(index_0, math.floor(len(index_1) * 1))\n",
    "samples = np.concatenate([index_1, index_0_comparable_to_1])\n",
    "print(str(len(index_1)) + ' + ' + str(len(index_0_comparable_to_1)) + ' = ' + str(len(samples)))\n",
    "\n",
    "\n",
    "small_data = data_pics.iloc[samples, :]\n",
    "small_target = target_pics.iloc[samples, :]\n",
    "\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(\n",
    "    small_data, small_target, test_size=0.3, random_state=randint(100, 10000))\n",
    "X_train_train_small, X_vali_small, y_train_train_small, y_vali_small = train_test_split(\n",
    "    X_train_small, y_train_small, test_size=0.3, random_state=randint(100, 10000))\n",
    "\n",
    "y_train_small_m = np.ravel(y_train_small)\n",
    "y_test_small_m = np.ravel(y_test_small)\n",
    "y_train_train_small_m = np.ravel(y_train_train_small)\n",
    "y_vali_small_m = np.ravel(y_vali_small)\n",
    "y_train_m = np.ravel(y_train)\n",
    "y_test_m = np.ravel(y_test)\n",
    "y_train_train_m = np.ravel(y_train_train)\n",
    "y_vali_m = np.ravel(y_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([15807, 15807], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = small_target.values.tolist()\n",
    "x = np.ravel(x)\n",
    "print(x)\n",
    "np.bincount(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.02770430\n",
      "Iteration 2, loss = 0.70144049\n",
      "Iteration 3, loss = 0.69450315\n",
      "Iteration 4, loss = 0.68838857\n",
      "Iteration 5, loss = 0.67941793\n",
      "Iteration 6, loss = 0.67723506\n",
      "Iteration 7, loss = 0.67285093\n",
      "Iteration 8, loss = 0.66934380\n",
      "Iteration 9, loss = 0.66960913\n",
      "Iteration 10, loss = 0.66610026\n",
      "Iteration 11, loss = 0.66469358\n",
      "Iteration 12, loss = 0.66399918\n",
      "Iteration 13, loss = 0.65724713\n",
      "Iteration 14, loss = 0.65467178\n",
      "Iteration 15, loss = 0.65421053\n",
      "Iteration 16, loss = 0.64871810\n",
      "Iteration 17, loss = 0.64999060\n",
      "Iteration 18, loss = 0.64378995\n",
      "Iteration 19, loss = 0.63733082\n",
      "Iteration 20, loss = 0.64195138\n",
      "Iteration 21, loss = 0.63393214\n",
      "Iteration 22, loss = 0.62319162\n",
      "Iteration 23, loss = 0.61872173\n",
      "Iteration 24, loss = 0.61677299\n",
      "Iteration 25, loss = 0.61225004\n",
      "Iteration 26, loss = 0.60522085\n",
      "Iteration 27, loss = 0.61490634\n",
      "Iteration 28, loss = 0.60604149\n",
      "Iteration 29, loss = 0.59713972\n",
      "Iteration 30, loss = 0.59505286\n",
      "Iteration 31, loss = 0.58506758\n",
      "Iteration 32, loss = 0.58375607\n",
      "Iteration 33, loss = 0.58905628\n",
      "Iteration 34, loss = 0.57903010\n",
      "Iteration 35, loss = 0.57651815\n",
      "Iteration 36, loss = 0.57050288\n",
      "Iteration 37, loss = 0.56816297\n",
      "Iteration 38, loss = 0.57023553\n",
      "Iteration 39, loss = 0.56838587\n",
      "Iteration 40, loss = 0.55796467\n",
      "Iteration 41, loss = 0.55247347\n",
      "Iteration 42, loss = 0.56247604\n",
      "Iteration 43, loss = 0.56757855\n",
      "Iteration 44, loss = 0.56266736\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6476541908276225"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc_st = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32, 16),\n",
    "                        alpha=0.15, max_iter=1000, batch_size=5000,\n",
    "                        verbose=True, learning_rate_init=0.04, tol=1e-5,\n",
    "                        learning_rate='adaptive')\n",
    "\n",
    "mlpc_st.fit(X_train_small, y_train_small_m)\n",
    "mlpc_st.score(X_test_small, y_test_small_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hlu82\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.495332004615546"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=10.0, verbose=True, max_iter=20000)\n",
    "lsvc.fit(X_train, y_train_m)\n",
    "lsvc.score(X_test_small, y_test_small_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 20.43, NNZs: 82, Bias: 1.925802, T: 24617, Avg. loss: 5.558488\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.15, NNZs: 82, Bias: 1.615312, T: 49234, Avg. loss: 1.772482\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.68, NNZs: 82, Bias: 0.525829, T: 73851, Avg. loss: 1.379800\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.70, NNZs: 82, Bias: 0.967571, T: 98468, Avg. loss: 1.217659\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.72, NNZs: 82, Bias: 0.721714, T: 123085, Avg. loss: 1.132471\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 8.22, NNZs: 82, Bias: 0.238330, T: 147702, Avg. loss: 1.078013\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 7.83, NNZs: 83, Bias: 0.863250, T: 172319, Avg. loss: 1.040861\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 7.46, NNZs: 83, Bias: 1.135296, T: 196936, Avg. loss: 1.012425\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 7.33, NNZs: 83, Bias: 0.585138, T: 221553, Avg. loss: 0.990004\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 6.93, NNZs: 83, Bias: 0.751894, T: 246170, Avg. loss: 0.976595\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 6.91, NNZs: 83, Bias: 0.484638, T: 270787, Avg. loss: 0.964878\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 6.73, NNZs: 83, Bias: 0.733955, T: 295404, Avg. loss: 0.950506\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 6.70, NNZs: 83, Bias: 0.569085, T: 320021, Avg. loss: 0.942410\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 6.41, NNZs: 83, Bias: 0.605197, T: 344638, Avg. loss: 0.938070\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 6.52, NNZs: 83, Bias: 0.553748, T: 369255, Avg. loss: 0.926168\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 6.36, NNZs: 83, Bias: 0.711202, T: 393872, Avg. loss: 0.923767\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 6.23, NNZs: 83, Bias: 1.038549, T: 418489, Avg. loss: 0.918465\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 6.17, NNZs: 83, Bias: 0.678641, T: 443106, Avg. loss: 0.911769\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 6.10, NNZs: 83, Bias: 0.656358, T: 467723, Avg. loss: 0.909526\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 6.13, NNZs: 83, Bias: 0.534577, T: 492340, Avg. loss: 0.906020\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 6.04, NNZs: 83, Bias: 0.501439, T: 516957, Avg. loss: 0.904294\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 5.98, NNZs: 83, Bias: 0.803391, T: 541574, Avg. loss: 0.898583\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 5.92, NNZs: 83, Bias: 0.675619, T: 566191, Avg. loss: 0.894806\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 5.93, NNZs: 83, Bias: 0.490657, T: 590808, Avg. loss: 0.890025\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 5.80, NNZs: 83, Bias: 0.594853, T: 615425, Avg. loss: 0.892437\n",
      "Total training time: 0.53 seconds.\n",
      "Convergence after 25 epochs took 0.54 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5948251350582883"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdc = SGDClassifier(verbose=True, max_iter=20000, tol=1e-4, penalty='l2', learning_rate='optimal')\n",
    "sgdc.fit(X_train_small, y_train_small_m)\n",
    "sgdc.score(X_test_small, y_test_small_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26838, 100)\n",
      "(24154, 100)\n",
      "class 0 has 21277 points\n",
      "class 1 has 2877 points\n",
      "2877 + 2877 = 5754\n",
      "Wall time: 227 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_dir = os.path.join(os.curdir, 'Data', 'Score', 'Short', '400000_fire3_2freq', 'data_short.pkl')\n",
    "with open(data_dir, 'rb') as in_file:\n",
    "    ot = pickle.load(in_file)\n",
    "data_pics = ot['data']\n",
    "target_pics = ot['target']\n",
    "data_pics = data_pics.iloc[:, 0:100]\n",
    "print(data_pics.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_pics, target_pics, test_size=0.1, random_state=randint(100, 10000))\n",
    "print(X_train.shape)\n",
    "X_train_train, X_vali, y_train_train, y_vali = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=randint(100, 10000))\n",
    "print('class 0 has ' + str(len(y_train.index[y_train[0] == 0].tolist())) + ' points')\n",
    "print('class 1 has ' + str(len(y_train.index[y_train[0] == 1].tolist())) + ' points')\n",
    "\n",
    "index_0 = y_train.index[y_train[0] == 0].tolist()\n",
    "index_1 = y_train.index[y_train[0] != 0].tolist()\n",
    "index_0_comparable_to_1 = np.random.choice(index_0, math.floor(len(index_1) * 1))\n",
    "samples = np.concatenate([index_1, index_0_comparable_to_1])\n",
    "print(str(len(index_1)) + ' + ' + str(len(index_0_comparable_to_1)) + ' = ' + str(len(samples)))\n",
    "\n",
    "\n",
    "small_data = data_pics.iloc[samples, :]\n",
    "small_target = target_pics.iloc[samples, :]\n",
    "\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(\n",
    "    small_data, small_target, test_size=0.3, random_state=randint(100, 10000))\n",
    "X_train_train_small, X_vali_small, y_train_train_small, y_vali_small = train_test_split(\n",
    "    X_train_small, y_train_small, test_size=0.3, random_state=randint(100, 10000))\n",
    "\n",
    "y_train_small_m = np.ravel(y_train_small)\n",
    "y_test_small_m = np.ravel(y_test_small)\n",
    "y_train_train_small_m = np.ravel(y_train_train_small)\n",
    "y_vali_small_m = np.ravel(y_vali_small)\n",
    "y_train_m = np.ravel(y_train)\n",
    "y_test_m = np.ravel(y_test)\n",
    "y_train_train_m = np.ravel(y_train_train)\n",
    "y_vali_m = np.ravel(y_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2877, 2877], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = small_target.values.tolist()\n",
    "x = np.ravel(x)\n",
    "print(x)\n",
    "np.bincount(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72521196\n",
      "Iteration 2, loss = 0.71340345\n",
      "Iteration 3, loss = 0.69140436\n",
      "Iteration 4, loss = 0.68158306\n",
      "Iteration 5, loss = 0.68056051\n",
      "Iteration 6, loss = 0.67251556\n",
      "Iteration 7, loss = 0.67217255\n",
      "Iteration 8, loss = 0.66574584\n",
      "Iteration 9, loss = 0.66343303\n",
      "Iteration 10, loss = 0.66270274\n",
      "Iteration 11, loss = 0.67426010\n",
      "Iteration 12, loss = 0.66246277\n",
      "Iteration 13, loss = 0.66521179\n",
      "Iteration 14, loss = 0.66044589\n",
      "Iteration 15, loss = 0.66130202\n",
      "Iteration 16, loss = 0.66906146\n",
      "Iteration 17, loss = 0.66251445\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5981470758540822"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc_st = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32, 16, 8),\n",
    "                        alpha=0.15, max_iter=1000, batch_size=2000,\n",
    "                        verbose=True, learning_rate_init=0.02, tol=1e-5,\n",
    "                        learning_rate='adaptive')\n",
    "\n",
    "mlpc_st.fit(X_train_small, y_train_small_m)\n",
    "mlpc_st.score(X_test_small, y_test_small_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.12, NNZs: 82, Bias: 0.040360, T: 4027, Avg. loss: 0.974788\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.12, NNZs: 82, Bias: 0.038697, T: 8054, Avg. loss: 0.967525\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.12, NNZs: 82, Bias: 0.038351, T: 12081, Avg. loss: 0.966576\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.12, NNZs: 82, Bias: 0.038643, T: 16108, Avg. loss: 0.966247\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.12, NNZs: 82, Bias: 0.038398, T: 20135, Avg. loss: 0.966355\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 5 epochs took 0.04 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6004632310364795"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdc = SGDClassifier(verbose=True, max_iter=20000, tol=1e-5, alpha=2,\n",
    "                     penalty='l2', learning_rate='optimal', loss='squared_hinge')\n",
    "sgdc.fit(X_train_small, y_train_small_m)\n",
    "sgdc.score(X_test_small, y_test_small_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4012, 100)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_small_rp = X_train_small.reset_index(inplace=False, drop=True)\n",
    "y_train_small_rp = y_train_small.reset_index(inplace=False, drop=True)\n",
    "X_train_small_rp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225543, 100)\n",
      "(202988, 100)\n",
      "class 0 has 185417 points\n",
      "class 1 has 17571 points\n",
      "17571 + 17571 = 35142\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_dir = os.path.join(os.curdir, 'Data', 'Score', 'Short', '400000_fire3_2freq', 'data_short.pkl')\n",
    "with open(data_dir, 'rb') as in_file:\n",
    "    ot = pickle.load(in_file)\n",
    "data_pics = ot['data']\n",
    "target_pics = ot['target']\n",
    "\n",
    "data_pics = data_pics.iloc[:, 0:100]\n",
    "print(data_pics.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_pics, target_pics, test_size=0.1, random_state=randint(100, 10000))\n",
    "print(X_train.shape)\n",
    "X_train_train, X_vali, y_train_train, y_vali = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=randint(100, 10000))\n",
    "print('class 0 has ' + str(len(y_train.index[y_train[0] == 0].tolist())) + ' points')\n",
    "print('class 1 has ' + str(len(y_train.index[y_train[0] == 1].tolist())) + ' points')\n",
    "\n",
    "index_0 = y_train.index[y_train[0] == 0].tolist()\n",
    "index_1 = y_train.index[y_train[0] != 0].tolist()\n",
    "index_0_comparable_to_1 = np.random.choice(index_0, math.floor(len(index_1) * 1))\n",
    "samples = np.concatenate([index_1, index_0_comparable_to_1])\n",
    "print(str(len(index_1)) + ' + ' + str(len(index_0_comparable_to_1)) + ' = ' + str(len(samples)))\n",
    "\n",
    "\n",
    "small_data = data_pics.iloc[samples, :]\n",
    "small_target = target_pics.iloc[samples, :]\n",
    "\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(\n",
    "    small_data, small_target, test_size=0.3, random_state=randint(100, 10000))\n",
    "X_train_train_small, X_vali_small, y_train_train_small, y_vali_small = train_test_split(\n",
    "    X_train_small, y_train_small, test_size=0.3, random_state=randint(100, 10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pics_np = np.array(data_pics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 22, 37, 38, 40, 63, 64, 73, 87, 88]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pics.index[target_pics[0] == 1].tolist()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the correct way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = np.reshape(data_pics_np, (data_pics_np.shape[0], 5, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pics_np[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data_np = np.array(small_data)\n",
    "small_data_input = np.reshape(small_data_np, (small_data_np.shape[0], 5, 20, 1))\n",
    "small_target_input = np.array(small_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "iddd = 11\n",
    "plt.imshow(small_data_input[iddd].reshape(5, 20))\n",
    "plt.show()\n",
    "print(small_data_input[iddd].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small_np, X_test_small_np, y_train_small_np, y_test_small_np = train_test_split(\n",
    "    small_data_input, small_target_input, test_size=0.3, random_state=randint(100, 10000))\n",
    "X_train_train_small_np, X_vali_small_np, y_train_train_small_np, y_vali_small_np = train_test_split(\n",
    "    X_train_small_np, y_train_small_np, test_size=0.3, random_state=randint(100, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train_train_small_np = to_categorical(y_train_train_small_np)\n",
    "y_vali_small_np = to_categorical(y_vali_small_np)\n",
    "y_train_small_np = to_categorical(y_train_small_np)\n",
    "y_test_small_np = to_categorical(y_test_small_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17219 samples, validate on 7380 samples\n",
      "Epoch 1/100\n",
      "17219/17219 [==============================] - ETA: 3s - loss: 0.6948 - acc: 0.501 - ETA: 2s - loss: 0.6889 - acc: 0.544 - ETA: 1s - loss: 0.6861 - acc: 0.553 - ETA: 1s - loss: 0.6811 - acc: 0.566 - ETA: 0s - loss: 0.6790 - acc: 0.575 - ETA: 0s - loss: 0.6774 - acc: 0.581 - ETA: 0s - loss: 0.6759 - acc: 0.586 - ETA: 0s - loss: 0.6733 - acc: 0.590 - 2s 105us/step - loss: 0.6727 - acc: 0.5921 - val_loss: 0.6658 - val_acc: 0.6065\n",
      "Epoch 2/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6583 - acc: 0.612 - ETA: 1s - loss: 0.6555 - acc: 0.612 - ETA: 0s - loss: 0.6559 - acc: 0.616 - ETA: 0s - loss: 0.6571 - acc: 0.614 - ETA: 0s - loss: 0.6580 - acc: 0.611 - ETA: 0s - loss: 0.6563 - acc: 0.614 - ETA: 0s - loss: 0.6561 - acc: 0.614 - ETA: 0s - loss: 0.6566 - acc: 0.614 - 2s 92us/step - loss: 0.6556 - acc: 0.6172 - val_loss: 0.6603 - val_acc: 0.6111\n",
      "Epoch 3/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6561 - acc: 0.620 - ETA: 1s - loss: 0.6542 - acc: 0.624 - ETA: 0s - loss: 0.6504 - acc: 0.628 - ETA: 0s - loss: 0.6521 - acc: 0.626 - ETA: 0s - loss: 0.6513 - acc: 0.625 - ETA: 0s - loss: 0.6509 - acc: 0.625 - ETA: 0s - loss: 0.6504 - acc: 0.624 - ETA: 0s - loss: 0.6512 - acc: 0.623 - 2s 94us/step - loss: 0.6516 - acc: 0.6230 - val_loss: 0.6580 - val_acc: 0.6153\n",
      "Epoch 4/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6387 - acc: 0.638 - ETA: 1s - loss: 0.6430 - acc: 0.632 - ETA: 0s - loss: 0.6456 - acc: 0.626 - ETA: 0s - loss: 0.6470 - acc: 0.624 - ETA: 0s - loss: 0.6473 - acc: 0.625 - ETA: 0s - loss: 0.6473 - acc: 0.627 - ETA: 0s - loss: 0.6474 - acc: 0.628 - ETA: 0s - loss: 0.6472 - acc: 0.627 - 2s 89us/step - loss: 0.6479 - acc: 0.6267 - val_loss: 0.6547 - val_acc: 0.6154\n",
      "Epoch 5/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6479 - acc: 0.624 - ETA: 1s - loss: 0.6439 - acc: 0.635 - ETA: 0s - loss: 0.6453 - acc: 0.633 - ETA: 0s - loss: 0.6451 - acc: 0.631 - ETA: 0s - loss: 0.6445 - acc: 0.632 - ETA: 0s - loss: 0.6433 - acc: 0.635 - ETA: 0s - loss: 0.6444 - acc: 0.631 - ETA: 0s - loss: 0.6451 - acc: 0.629 - 2s 88us/step - loss: 0.6454 - acc: 0.6297 - val_loss: 0.6544 - val_acc: 0.6167\n",
      "Epoch 6/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6366 - acc: 0.646 - ETA: 1s - loss: 0.6419 - acc: 0.634 - ETA: 0s - loss: 0.6440 - acc: 0.628 - ETA: 0s - loss: 0.6427 - acc: 0.629 - ETA: 0s - loss: 0.6423 - acc: 0.629 - ETA: 0s - loss: 0.6433 - acc: 0.628 - ETA: 0s - loss: 0.6434 - acc: 0.629 - ETA: 0s - loss: 0.6430 - acc: 0.629 - 2s 88us/step - loss: 0.6435 - acc: 0.6290 - val_loss: 0.6517 - val_acc: 0.6157\n",
      "Epoch 7/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6491 - acc: 0.621 - ETA: 1s - loss: 0.6526 - acc: 0.619 - ETA: 0s - loss: 0.6477 - acc: 0.627 - ETA: 0s - loss: 0.6454 - acc: 0.627 - ETA: 0s - loss: 0.6445 - acc: 0.628 - ETA: 0s - loss: 0.6425 - acc: 0.631 - ETA: 0s - loss: 0.6415 - acc: 0.633 - ETA: 0s - loss: 0.6405 - acc: 0.635 - 2s 89us/step - loss: 0.6405 - acc: 0.6348 - val_loss: 0.6523 - val_acc: 0.6180\n",
      "Epoch 8/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6344 - acc: 0.638 - ETA: 1s - loss: 0.6346 - acc: 0.639 - ETA: 0s - loss: 0.6324 - acc: 0.641 - ETA: 0s - loss: 0.6335 - acc: 0.638 - ETA: 0s - loss: 0.6350 - acc: 0.635 - ETA: 0s - loss: 0.6374 - acc: 0.632 - ETA: 0s - loss: 0.6376 - acc: 0.632 - ETA: 0s - loss: 0.6382 - acc: 0.632 - 2s 87us/step - loss: 0.6379 - acc: 0.6320 - val_loss: 0.6516 - val_acc: 0.6140\n",
      "Epoch 9/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6453 - acc: 0.625 - ETA: 1s - loss: 0.6349 - acc: 0.636 - ETA: 0s - loss: 0.6355 - acc: 0.637 - ETA: 0s - loss: 0.6343 - acc: 0.635 - ETA: 0s - loss: 0.6339 - acc: 0.640 - ETA: 0s - loss: 0.6345 - acc: 0.639 - ETA: 0s - loss: 0.6346 - acc: 0.639 - ETA: 0s - loss: 0.6352 - acc: 0.638 - 2s 88us/step - loss: 0.6351 - acc: 0.6392 - val_loss: 0.6483 - val_acc: 0.6184\n",
      "Epoch 10/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6300 - acc: 0.642 - ETA: 0s - loss: 0.6296 - acc: 0.641 - ETA: 0s - loss: 0.6345 - acc: 0.632 - ETA: 0s - loss: 0.6321 - acc: 0.635 - ETA: 0s - loss: 0.6306 - acc: 0.638 - ETA: 0s - loss: 0.6307 - acc: 0.637 - ETA: 0s - loss: 0.6311 - acc: 0.637 - ETA: 0s - loss: 0.6308 - acc: 0.637 - 2s 88us/step - loss: 0.6314 - acc: 0.6361 - val_loss: 0.6458 - val_acc: 0.6161\n",
      "Epoch 11/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6253 - acc: 0.649 - ETA: 1s - loss: 0.6263 - acc: 0.645 - ETA: 0s - loss: 0.6279 - acc: 0.645 - ETA: 0s - loss: 0.6262 - acc: 0.648 - ETA: 0s - loss: 0.6264 - acc: 0.647 - ETA: 0s - loss: 0.6272 - acc: 0.644 - ETA: 0s - loss: 0.6283 - acc: 0.642 - ETA: 0s - loss: 0.6281 - acc: 0.642 - 2s 90us/step - loss: 0.6288 - acc: 0.6412 - val_loss: 0.6444 - val_acc: 0.6178\n",
      "Epoch 12/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6195 - acc: 0.650 - ETA: 1s - loss: 0.6239 - acc: 0.647 - ETA: 0s - loss: 0.6226 - acc: 0.648 - ETA: 0s - loss: 0.6224 - acc: 0.645 - ETA: 0s - loss: 0.6240 - acc: 0.645 - ETA: 0s - loss: 0.6246 - acc: 0.643 - ETA: 0s - loss: 0.6253 - acc: 0.642 - ETA: 0s - loss: 0.6256 - acc: 0.641 - 1s 86us/step - loss: 0.6253 - acc: 0.6420 - val_loss: 0.6429 - val_acc: 0.6176\n",
      "Epoch 13/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6258 - acc: 0.636 - ETA: 1s - loss: 0.6279 - acc: 0.642 - ETA: 0s - loss: 0.6240 - acc: 0.645 - ETA: 0s - loss: 0.6232 - acc: 0.644 - ETA: 0s - loss: 0.6229 - acc: 0.644 - ETA: 0s - loss: 0.6236 - acc: 0.644 - ETA: 0s - loss: 0.6224 - acc: 0.645 - ETA: 0s - loss: 0.6220 - acc: 0.647 - 1s 87us/step - loss: 0.6216 - acc: 0.6471 - val_loss: 0.6404 - val_acc: 0.6206\n",
      "Epoch 14/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6197 - acc: 0.638 - ETA: 1s - loss: 0.6208 - acc: 0.641 - ETA: 0s - loss: 0.6178 - acc: 0.646 - ETA: 0s - loss: 0.6188 - acc: 0.648 - ETA: 0s - loss: 0.6185 - acc: 0.648 - ETA: 0s - loss: 0.6194 - acc: 0.647 - ETA: 0s - loss: 0.6193 - acc: 0.647 - ETA: 0s - loss: 0.6180 - acc: 0.648 - 2s 89us/step - loss: 0.6175 - acc: 0.6484 - val_loss: 0.6382 - val_acc: 0.6154\n",
      "Epoch 15/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6268 - acc: 0.645 - ETA: 1s - loss: 0.6224 - acc: 0.641 - ETA: 0s - loss: 0.6164 - acc: 0.649 - ETA: 0s - loss: 0.6165 - acc: 0.649 - ETA: 0s - loss: 0.6167 - acc: 0.647 - ETA: 0s - loss: 0.6157 - acc: 0.645 - ETA: 0s - loss: 0.6149 - acc: 0.647 - ETA: 0s - loss: 0.6141 - acc: 0.648 - 1s 87us/step - loss: 0.6138 - acc: 0.6488 - val_loss: 0.6403 - val_acc: 0.6180\n",
      "Epoch 16/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6195 - acc: 0.638 - ETA: 1s - loss: 0.6117 - acc: 0.645 - ETA: 0s - loss: 0.6077 - acc: 0.654 - ETA: 0s - loss: 0.6090 - acc: 0.652 - ETA: 0s - loss: 0.6072 - acc: 0.655 - ETA: 0s - loss: 0.6090 - acc: 0.653 - ETA: 0s - loss: 0.6115 - acc: 0.649 - ETA: 0s - loss: 0.6109 - acc: 0.650 - 2s 88us/step - loss: 0.6105 - acc: 0.6506 - val_loss: 0.6385 - val_acc: 0.6175\n",
      "Epoch 17/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6003 - acc: 0.668 - ETA: 0s - loss: 0.6024 - acc: 0.664 - ETA: 0s - loss: 0.6081 - acc: 0.656 - ETA: 0s - loss: 0.6099 - acc: 0.655 - ETA: 0s - loss: 0.6072 - acc: 0.658 - ETA: 0s - loss: 0.6077 - acc: 0.656 - ETA: 0s - loss: 0.6080 - acc: 0.656 - ETA: 0s - loss: 0.6083 - acc: 0.655 - 1s 86us/step - loss: 0.6078 - acc: 0.6555 - val_loss: 0.6370 - val_acc: 0.6165\n",
      "Epoch 18/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6082 - acc: 0.668 - ETA: 1s - loss: 0.6021 - acc: 0.673 - ETA: 0s - loss: 0.6025 - acc: 0.666 - ETA: 0s - loss: 0.6039 - acc: 0.663 - ETA: 0s - loss: 0.6041 - acc: 0.661 - ETA: 0s - loss: 0.6050 - acc: 0.659 - ETA: 0s - loss: 0.6042 - acc: 0.659 - ETA: 0s - loss: 0.6033 - acc: 0.659 - 2s 90us/step - loss: 0.6034 - acc: 0.6592 - val_loss: 0.6361 - val_acc: 0.6172\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17219/17219 [==============================] - ETA: 1s - loss: 0.6009 - acc: 0.667 - ETA: 0s - loss: 0.6029 - acc: 0.662 - ETA: 0s - loss: 0.5995 - acc: 0.662 - ETA: 0s - loss: 0.5980 - acc: 0.664 - ETA: 0s - loss: 0.5984 - acc: 0.662 - ETA: 0s - loss: 0.5986 - acc: 0.661 - ETA: 0s - loss: 0.5989 - acc: 0.661 - ETA: 0s - loss: 0.5991 - acc: 0.661 - 1s 86us/step - loss: 0.5999 - acc: 0.6603 - val_loss: 0.6373 - val_acc: 0.6164\n",
      "Epoch 20/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5978 - acc: 0.664 - ETA: 1s - loss: 0.5970 - acc: 0.660 - ETA: 0s - loss: 0.5936 - acc: 0.663 - ETA: 0s - loss: 0.5936 - acc: 0.665 - ETA: 0s - loss: 0.5955 - acc: 0.663 - ETA: 0s - loss: 0.5955 - acc: 0.665 - ETA: 0s - loss: 0.5973 - acc: 0.661 - ETA: 0s - loss: 0.5980 - acc: 0.661 - 2s 89us/step - loss: 0.5983 - acc: 0.6616 - val_loss: 0.6381 - val_acc: 0.6182\n",
      "Epoch 21/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5743 - acc: 0.687 - ETA: 1s - loss: 0.5835 - acc: 0.670 - ETA: 0s - loss: 0.5906 - acc: 0.661 - ETA: 0s - loss: 0.5945 - acc: 0.659 - ETA: 0s - loss: 0.5935 - acc: 0.662 - ETA: 0s - loss: 0.5950 - acc: 0.660 - ETA: 0s - loss: 0.5954 - acc: 0.659 - ETA: 0s - loss: 0.5961 - acc: 0.659 - 1s 87us/step - loss: 0.5961 - acc: 0.6601 - val_loss: 0.6380 - val_acc: 0.6161\n",
      "Epoch 22/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5911 - acc: 0.660 - ETA: 1s - loss: 0.5887 - acc: 0.668 - ETA: 0s - loss: 0.5876 - acc: 0.667 - ETA: 0s - loss: 0.5901 - acc: 0.664 - ETA: 0s - loss: 0.5913 - acc: 0.663 - ETA: 0s - loss: 0.5923 - acc: 0.660 - ETA: 0s - loss: 0.5942 - acc: 0.658 - ETA: 0s - loss: 0.5936 - acc: 0.660 - 2s 89us/step - loss: 0.5931 - acc: 0.6617 - val_loss: 0.6385 - val_acc: 0.6115\n",
      "Epoch 23/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5901 - acc: 0.663 - ETA: 1s - loss: 0.5823 - acc: 0.673 - ETA: 0s - loss: 0.5821 - acc: 0.673 - ETA: 0s - loss: 0.5865 - acc: 0.669 - ETA: 0s - loss: 0.5876 - acc: 0.670 - ETA: 0s - loss: 0.5877 - acc: 0.670 - ETA: 0s - loss: 0.5882 - acc: 0.669 - ETA: 0s - loss: 0.5893 - acc: 0.668 - 1s 87us/step - loss: 0.5907 - acc: 0.6665 - val_loss: 0.6397 - val_acc: 0.6115\n",
      "Epoch 24/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5921 - acc: 0.667 - ETA: 1s - loss: 0.5860 - acc: 0.669 - ETA: 0s - loss: 0.5859 - acc: 0.670 - ETA: 0s - loss: 0.5882 - acc: 0.666 - ETA: 0s - loss: 0.5876 - acc: 0.666 - ETA: 0s - loss: 0.5881 - acc: 0.665 - ETA: 0s - loss: 0.5874 - acc: 0.668 - ETA: 0s - loss: 0.5871 - acc: 0.668 - 2s 90us/step - loss: 0.5873 - acc: 0.6683 - val_loss: 0.6402 - val_acc: 0.6115\n",
      "Epoch 25/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5921 - acc: 0.665 - ETA: 1s - loss: 0.5918 - acc: 0.666 - ETA: 0s - loss: 0.5852 - acc: 0.673 - ETA: 0s - loss: 0.5830 - acc: 0.672 - ETA: 0s - loss: 0.5810 - acc: 0.674 - ETA: 0s - loss: 0.5845 - acc: 0.672 - ETA: 0s - loss: 0.5838 - acc: 0.673 - ETA: 0s - loss: 0.5845 - acc: 0.672 - 2s 88us/step - loss: 0.5851 - acc: 0.6714 - val_loss: 0.6436 - val_acc: 0.6049\n",
      "Epoch 26/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5786 - acc: 0.666 - ETA: 1s - loss: 0.5798 - acc: 0.670 - ETA: 0s - loss: 0.5763 - acc: 0.676 - ETA: 0s - loss: 0.5798 - acc: 0.677 - ETA: 0s - loss: 0.5811 - acc: 0.675 - ETA: 0s - loss: 0.5824 - acc: 0.674 - ETA: 0s - loss: 0.5830 - acc: 0.674 - ETA: 0s - loss: 0.5840 - acc: 0.674 - 1s 87us/step - loss: 0.5834 - acc: 0.6747 - val_loss: 0.6428 - val_acc: 0.6091\n",
      "Epoch 27/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5831 - acc: 0.671 - ETA: 0s - loss: 0.5783 - acc: 0.675 - ETA: 0s - loss: 0.5781 - acc: 0.673 - ETA: 0s - loss: 0.5800 - acc: 0.675 - ETA: 0s - loss: 0.5820 - acc: 0.672 - ETA: 0s - loss: 0.5808 - acc: 0.673 - ETA: 0s - loss: 0.5807 - acc: 0.673 - ETA: 0s - loss: 0.5815 - acc: 0.673 - 1s 85us/step - loss: 0.5816 - acc: 0.6726 - val_loss: 0.6452 - val_acc: 0.6050\n",
      "Epoch 28/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5817 - acc: 0.674 - ETA: 1s - loss: 0.5782 - acc: 0.678 - ETA: 0s - loss: 0.5761 - acc: 0.678 - ETA: 0s - loss: 0.5783 - acc: 0.676 - ETA: 0s - loss: 0.5805 - acc: 0.674 - ETA: 0s - loss: 0.5813 - acc: 0.675 - ETA: 0s - loss: 0.5809 - acc: 0.676 - ETA: 0s - loss: 0.5808 - acc: 0.675 - 1s 87us/step - loss: 0.5807 - acc: 0.6756 - val_loss: 0.6505 - val_acc: 0.5930\n",
      "Epoch 29/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5668 - acc: 0.681 - ETA: 1s - loss: 0.5709 - acc: 0.679 - ETA: 0s - loss: 0.5729 - acc: 0.677 - ETA: 0s - loss: 0.5721 - acc: 0.681 - ETA: 0s - loss: 0.5726 - acc: 0.681 - ETA: 0s - loss: 0.5749 - acc: 0.679 - ETA: 0s - loss: 0.5768 - acc: 0.678 - ETA: 0s - loss: 0.5785 - acc: 0.676 - 2s 89us/step - loss: 0.5790 - acc: 0.6764 - val_loss: 0.6447 - val_acc: 0.6034\n",
      "Epoch 30/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5730 - acc: 0.674 - ETA: 1s - loss: 0.5720 - acc: 0.680 - ETA: 0s - loss: 0.5768 - acc: 0.677 - ETA: 0s - loss: 0.5764 - acc: 0.678 - ETA: 0s - loss: 0.5739 - acc: 0.680 - ETA: 0s - loss: 0.5764 - acc: 0.677 - ETA: 0s - loss: 0.5786 - acc: 0.676 - ETA: 0s - loss: 0.5782 - acc: 0.676 - 2s 89us/step - loss: 0.5785 - acc: 0.6763 - val_loss: 0.6489 - val_acc: 0.6098\n",
      "Epoch 31/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5752 - acc: 0.677 - ETA: 1s - loss: 0.5729 - acc: 0.680 - ETA: 0s - loss: 0.5757 - acc: 0.678 - ETA: 0s - loss: 0.5806 - acc: 0.675 - ETA: 0s - loss: 0.5762 - acc: 0.681 - ETA: 0s - loss: 0.5744 - acc: 0.683 - ETA: 0s - loss: 0.5772 - acc: 0.680 - ETA: 0s - loss: 0.5764 - acc: 0.681 - 2s 89us/step - loss: 0.5768 - acc: 0.6806 - val_loss: 0.6478 - val_acc: 0.6024\n",
      "Epoch 32/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5826 - acc: 0.681 - ETA: 1s - loss: 0.5774 - acc: 0.679 - ETA: 0s - loss: 0.5752 - acc: 0.680 - ETA: 0s - loss: 0.5722 - acc: 0.681 - ETA: 0s - loss: 0.5731 - acc: 0.681 - ETA: 0s - loss: 0.5750 - acc: 0.679 - ETA: 0s - loss: 0.5749 - acc: 0.679 - ETA: 0s - loss: 0.5747 - acc: 0.679 - 2s 94us/step - loss: 0.5745 - acc: 0.6799 - val_loss: 0.6531 - val_acc: 0.5989\n",
      "Epoch 33/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5643 - acc: 0.690 - ETA: 0s - loss: 0.5654 - acc: 0.683 - ETA: 0s - loss: 0.5647 - acc: 0.686 - ETA: 0s - loss: 0.5635 - acc: 0.690 - ETA: 0s - loss: 0.5688 - acc: 0.685 - ETA: 0s - loss: 0.5712 - acc: 0.682 - ETA: 0s - loss: 0.5721 - acc: 0.682 - ETA: 0s - loss: 0.5742 - acc: 0.679 - 1s 86us/step - loss: 0.5745 - acc: 0.6806 - val_loss: 0.6495 - val_acc: 0.6020\n",
      "Epoch 34/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5689 - acc: 0.680 - ETA: 0s - loss: 0.5769 - acc: 0.672 - ETA: 0s - loss: 0.5729 - acc: 0.679 - ETA: 0s - loss: 0.5682 - acc: 0.684 - ETA: 0s - loss: 0.5685 - acc: 0.683 - ETA: 0s - loss: 0.5677 - acc: 0.683 - ETA: 0s - loss: 0.5704 - acc: 0.681 - ETA: 0s - loss: 0.5697 - acc: 0.682 - 1s 87us/step - loss: 0.5693 - acc: 0.6839 - val_loss: 0.6503 - val_acc: 0.5995\n",
      "Epoch 35/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5737 - acc: 0.667 - ETA: 1s - loss: 0.5660 - acc: 0.683 - ETA: 0s - loss: 0.5683 - acc: 0.687 - ETA: 0s - loss: 0.5695 - acc: 0.685 - ETA: 0s - loss: 0.5679 - acc: 0.687 - ETA: 0s - loss: 0.5669 - acc: 0.686 - ETA: 0s - loss: 0.5685 - acc: 0.684 - ETA: 0s - loss: 0.5686 - acc: 0.683 - 2s 92us/step - loss: 0.5686 - acc: 0.6833 - val_loss: 0.6526 - val_acc: 0.6037\n",
      "Epoch 36/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5651 - acc: 0.688 - ETA: 1s - loss: 0.5670 - acc: 0.684 - ETA: 0s - loss: 0.5691 - acc: 0.684 - ETA: 0s - loss: 0.5676 - acc: 0.688 - ETA: 0s - loss: 0.5661 - acc: 0.688 - ETA: 0s - loss: 0.5670 - acc: 0.687 - ETA: 0s - loss: 0.5682 - acc: 0.685 - ETA: 0s - loss: 0.5682 - acc: 0.684 - 2s 88us/step - loss: 0.5684 - acc: 0.6848 - val_loss: 0.6535 - val_acc: 0.5962\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5586 - acc: 0.691 - ETA: 1s - loss: 0.5548 - acc: 0.697 - ETA: 0s - loss: 0.5623 - acc: 0.690 - ETA: 0s - loss: 0.5616 - acc: 0.693 - ETA: 0s - loss: 0.5639 - acc: 0.690 - ETA: 0s - loss: 0.5627 - acc: 0.689 - ETA: 0s - loss: 0.5661 - acc: 0.686 - ETA: 0s - loss: 0.5672 - acc: 0.684 - 1s 86us/step - loss: 0.5672 - acc: 0.6846 - val_loss: 0.6545 - val_acc: 0.6034\n",
      "Epoch 38/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5525 - acc: 0.693 - ETA: 1s - loss: 0.5574 - acc: 0.689 - ETA: 0s - loss: 0.5587 - acc: 0.690 - ETA: 0s - loss: 0.5609 - acc: 0.690 - ETA: 0s - loss: 0.5633 - acc: 0.691 - ETA: 0s - loss: 0.5643 - acc: 0.689 - ETA: 0s - loss: 0.5657 - acc: 0.689 - ETA: 0s - loss: 0.5672 - acc: 0.688 - 1s 85us/step - loss: 0.5680 - acc: 0.6876 - val_loss: 0.6566 - val_acc: 0.6056\n",
      "Epoch 39/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5643 - acc: 0.694 - ETA: 0s - loss: 0.5617 - acc: 0.694 - ETA: 0s - loss: 0.5602 - acc: 0.692 - ETA: 0s - loss: 0.5619 - acc: 0.691 - ETA: 0s - loss: 0.5603 - acc: 0.694 - ETA: 0s - loss: 0.5622 - acc: 0.693 - ETA: 0s - loss: 0.5622 - acc: 0.691 - ETA: 0s - loss: 0.5642 - acc: 0.690 - 1s 84us/step - loss: 0.5655 - acc: 0.6887 - val_loss: 0.6554 - val_acc: 0.5985\n",
      "Epoch 40/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5518 - acc: 0.700 - ETA: 1s - loss: 0.5524 - acc: 0.700 - ETA: 0s - loss: 0.5579 - acc: 0.692 - ETA: 0s - loss: 0.5603 - acc: 0.692 - ETA: 0s - loss: 0.5614 - acc: 0.691 - ETA: 0s - loss: 0.5639 - acc: 0.689 - ETA: 0s - loss: 0.5617 - acc: 0.690 - ETA: 0s - loss: 0.5631 - acc: 0.689 - 1s 86us/step - loss: 0.5639 - acc: 0.6891 - val_loss: 0.6548 - val_acc: 0.6019\n",
      "Epoch 41/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5648 - acc: 0.696 - ETA: 0s - loss: 0.5572 - acc: 0.704 - ETA: 0s - loss: 0.5567 - acc: 0.700 - ETA: 0s - loss: 0.5587 - acc: 0.697 - ETA: 0s - loss: 0.5600 - acc: 0.697 - ETA: 0s - loss: 0.5585 - acc: 0.698 - ETA: 0s - loss: 0.5600 - acc: 0.695 - ETA: 0s - loss: 0.5615 - acc: 0.693 - 2s 89us/step - loss: 0.5619 - acc: 0.6922 - val_loss: 0.6558 - val_acc: 0.6007\n",
      "Epoch 42/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5746 - acc: 0.679 - ETA: 1s - loss: 0.5610 - acc: 0.688 - ETA: 0s - loss: 0.5582 - acc: 0.696 - ETA: 0s - loss: 0.5553 - acc: 0.699 - ETA: 0s - loss: 0.5588 - acc: 0.693 - ETA: 0s - loss: 0.5594 - acc: 0.691 - ETA: 0s - loss: 0.5599 - acc: 0.690 - ETA: 0s - loss: 0.5600 - acc: 0.691 - 2s 91us/step - loss: 0.5611 - acc: 0.6909 - val_loss: 0.6564 - val_acc: 0.6073\n",
      "Epoch 43/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5562 - acc: 0.690 - ETA: 1s - loss: 0.5582 - acc: 0.692 - ETA: 0s - loss: 0.5622 - acc: 0.686 - ETA: 0s - loss: 0.5645 - acc: 0.685 - ETA: 0s - loss: 0.5628 - acc: 0.685 - ETA: 0s - loss: 0.5649 - acc: 0.683 - ETA: 0s - loss: 0.5656 - acc: 0.685 - ETA: 0s - loss: 0.5662 - acc: 0.685 - 2s 92us/step - loss: 0.5652 - acc: 0.6868 - val_loss: 0.6598 - val_acc: 0.5982\n",
      "Epoch 44/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5519 - acc: 0.707 - ETA: 1s - loss: 0.5527 - acc: 0.706 - ETA: 0s - loss: 0.5546 - acc: 0.706 - ETA: 0s - loss: 0.5532 - acc: 0.702 - ETA: 0s - loss: 0.5543 - acc: 0.701 - ETA: 0s - loss: 0.5576 - acc: 0.697 - ETA: 0s - loss: 0.5588 - acc: 0.696 - ETA: 0s - loss: 0.5595 - acc: 0.694 - 1s 85us/step - loss: 0.5587 - acc: 0.6955 - val_loss: 0.6574 - val_acc: 0.6042\n",
      "Epoch 45/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5580 - acc: 0.690 - ETA: 0s - loss: 0.5545 - acc: 0.697 - ETA: 0s - loss: 0.5587 - acc: 0.694 - ETA: 0s - loss: 0.5624 - acc: 0.688 - ETA: 0s - loss: 0.5608 - acc: 0.692 - ETA: 0s - loss: 0.5604 - acc: 0.691 - ETA: 0s - loss: 0.5589 - acc: 0.695 - ETA: 0s - loss: 0.5596 - acc: 0.695 - 1s 84us/step - loss: 0.5595 - acc: 0.6949 - val_loss: 0.6599 - val_acc: 0.6050\n",
      "Epoch 46/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5457 - acc: 0.692 - ETA: 1s - loss: 0.5452 - acc: 0.699 - ETA: 0s - loss: 0.5513 - acc: 0.693 - ETA: 0s - loss: 0.5536 - acc: 0.693 - ETA: 0s - loss: 0.5551 - acc: 0.693 - ETA: 0s - loss: 0.5540 - acc: 0.693 - ETA: 0s - loss: 0.5537 - acc: 0.694 - ETA: 0s - loss: 0.5572 - acc: 0.692 - 1s 85us/step - loss: 0.5583 - acc: 0.6919 - val_loss: 0.6589 - val_acc: 0.6019\n",
      "Epoch 47/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5443 - acc: 0.715 - ETA: 1s - loss: 0.5495 - acc: 0.705 - ETA: 0s - loss: 0.5543 - acc: 0.702 - ETA: 0s - loss: 0.5575 - acc: 0.695 - ETA: 0s - loss: 0.5560 - acc: 0.695 - ETA: 0s - loss: 0.5567 - acc: 0.694 - ETA: 0s - loss: 0.5579 - acc: 0.694 - ETA: 0s - loss: 0.5577 - acc: 0.694 - 1s 84us/step - loss: 0.5570 - acc: 0.6950 - val_loss: 0.6603 - val_acc: 0.6077\n",
      "Epoch 48/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5523 - acc: 0.689 - ETA: 0s - loss: 0.5509 - acc: 0.701 - ETA: 0s - loss: 0.5452 - acc: 0.705 - ETA: 0s - loss: 0.5491 - acc: 0.702 - ETA: 0s - loss: 0.5496 - acc: 0.698 - ETA: 0s - loss: 0.5484 - acc: 0.700 - ETA: 0s - loss: 0.5522 - acc: 0.697 - ETA: 0s - loss: 0.5540 - acc: 0.696 - 1s 86us/step - loss: 0.5546 - acc: 0.6953 - val_loss: 0.6636 - val_acc: 0.6031\n",
      "Epoch 49/100\n",
      "17219/17219 [==============================] - ETA: 0s - loss: 0.5573 - acc: 0.692 - ETA: 0s - loss: 0.5515 - acc: 0.699 - ETA: 0s - loss: 0.5526 - acc: 0.697 - ETA: 0s - loss: 0.5532 - acc: 0.695 - ETA: 0s - loss: 0.5552 - acc: 0.694 - ETA: 0s - loss: 0.5550 - acc: 0.694 - ETA: 0s - loss: 0.5559 - acc: 0.694 - ETA: 0s - loss: 0.5542 - acc: 0.696 - 1s 85us/step - loss: 0.5548 - acc: 0.6960 - val_loss: 0.6633 - val_acc: 0.6075\n",
      "Epoch 50/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5311 - acc: 0.720 - ETA: 1s - loss: 0.5433 - acc: 0.709 - ETA: 0s - loss: 0.5451 - acc: 0.709 - ETA: 0s - loss: 0.5461 - acc: 0.707 - ETA: 0s - loss: 0.5480 - acc: 0.704 - ETA: 0s - loss: 0.5505 - acc: 0.700 - ETA: 0s - loss: 0.5532 - acc: 0.697 - ETA: 0s - loss: 0.5539 - acc: 0.696 - 1s 86us/step - loss: 0.5536 - acc: 0.6966 - val_loss: 0.6641 - val_acc: 0.6042\n",
      "Epoch 51/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5467 - acc: 0.710 - ETA: 0s - loss: 0.5461 - acc: 0.705 - ETA: 0s - loss: 0.5494 - acc: 0.704 - ETA: 0s - loss: 0.5518 - acc: 0.703 - ETA: 0s - loss: 0.5530 - acc: 0.702 - ETA: 0s - loss: 0.5520 - acc: 0.703 - ETA: 0s - loss: 0.5497 - acc: 0.705 - ETA: 0s - loss: 0.5524 - acc: 0.703 - 1s 86us/step - loss: 0.5524 - acc: 0.7031 - val_loss: 0.6646 - val_acc: 0.6042\n",
      "Epoch 52/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5410 - acc: 0.712 - ETA: 0s - loss: 0.5380 - acc: 0.709 - ETA: 0s - loss: 0.5415 - acc: 0.705 - ETA: 0s - loss: 0.5493 - acc: 0.699 - ETA: 0s - loss: 0.5493 - acc: 0.697 - ETA: 0s - loss: 0.5508 - acc: 0.698 - ETA: 0s - loss: 0.5516 - acc: 0.697 - ETA: 0s - loss: 0.5522 - acc: 0.696 - 1s 82us/step - loss: 0.5516 - acc: 0.6979 - val_loss: 0.6651 - val_acc: 0.6056\n",
      "Epoch 53/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5410 - acc: 0.704 - ETA: 0s - loss: 0.5510 - acc: 0.700 - ETA: 0s - loss: 0.5450 - acc: 0.706 - ETA: 0s - loss: 0.5460 - acc: 0.704 - ETA: 0s - loss: 0.5496 - acc: 0.701 - ETA: 0s - loss: 0.5488 - acc: 0.703 - ETA: 0s - loss: 0.5482 - acc: 0.704 - ETA: 0s - loss: 0.5507 - acc: 0.701 - 1s 86us/step - loss: 0.5504 - acc: 0.7017 - val_loss: 0.6677 - val_acc: 0.5986\n",
      "Epoch 54/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5471 - acc: 0.690 - ETA: 1s - loss: 0.5474 - acc: 0.698 - ETA: 0s - loss: 0.5439 - acc: 0.704 - ETA: 0s - loss: 0.5483 - acc: 0.700 - ETA: 0s - loss: 0.5472 - acc: 0.701 - ETA: 0s - loss: 0.5477 - acc: 0.701 - ETA: 0s - loss: 0.5479 - acc: 0.701 - ETA: 0s - loss: 0.5496 - acc: 0.699 - 1s 85us/step - loss: 0.5505 - acc: 0.6991 - val_loss: 0.6687 - val_acc: 0.6045\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5587 - acc: 0.695 - ETA: 0s - loss: 0.5422 - acc: 0.709 - ETA: 0s - loss: 0.5442 - acc: 0.702 - ETA: 0s - loss: 0.5441 - acc: 0.703 - ETA: 0s - loss: 0.5451 - acc: 0.703 - ETA: 0s - loss: 0.5473 - acc: 0.703 - ETA: 0s - loss: 0.5477 - acc: 0.702 - ETA: 0s - loss: 0.5480 - acc: 0.702 - 1s 84us/step - loss: 0.5491 - acc: 0.7007 - val_loss: 0.6678 - val_acc: 0.5993\n",
      "Epoch 56/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5396 - acc: 0.707 - ETA: 0s - loss: 0.5434 - acc: 0.710 - ETA: 0s - loss: 0.5467 - acc: 0.703 - ETA: 0s - loss: 0.5448 - acc: 0.706 - ETA: 0s - loss: 0.5439 - acc: 0.709 - ETA: 0s - loss: 0.5478 - acc: 0.705 - ETA: 0s - loss: 0.5483 - acc: 0.704 - ETA: 0s - loss: 0.5506 - acc: 0.701 - 1s 85us/step - loss: 0.5498 - acc: 0.7021 - val_loss: 0.6696 - val_acc: 0.6041\n",
      "Epoch 57/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5600 - acc: 0.694 - ETA: 0s - loss: 0.5509 - acc: 0.700 - ETA: 0s - loss: 0.5524 - acc: 0.699 - ETA: 0s - loss: 0.5497 - acc: 0.702 - ETA: 0s - loss: 0.5472 - acc: 0.705 - ETA: 0s - loss: 0.5460 - acc: 0.705 - ETA: 0s - loss: 0.5471 - acc: 0.703 - ETA: 0s - loss: 0.5483 - acc: 0.702 - 1s 83us/step - loss: 0.5478 - acc: 0.7033 - val_loss: 0.6715 - val_acc: 0.6057\n",
      "Epoch 58/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5561 - acc: 0.697 - ETA: 0s - loss: 0.5503 - acc: 0.701 - ETA: 0s - loss: 0.5528 - acc: 0.699 - ETA: 0s - loss: 0.5461 - acc: 0.707 - ETA: 0s - loss: 0.5438 - acc: 0.707 - ETA: 0s - loss: 0.5433 - acc: 0.706 - ETA: 0s - loss: 0.5458 - acc: 0.704 - ETA: 0s - loss: 0.5468 - acc: 0.703 - 1s 83us/step - loss: 0.5462 - acc: 0.7039 - val_loss: 0.6716 - val_acc: 0.5980\n",
      "Epoch 59/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5491 - acc: 0.697 - ETA: 0s - loss: 0.5469 - acc: 0.701 - ETA: 0s - loss: 0.5501 - acc: 0.700 - ETA: 0s - loss: 0.5458 - acc: 0.703 - ETA: 0s - loss: 0.5463 - acc: 0.702 - ETA: 0s - loss: 0.5475 - acc: 0.701 - ETA: 0s - loss: 0.5473 - acc: 0.702 - ETA: 0s - loss: 0.5467 - acc: 0.703 - 1s 83us/step - loss: 0.5464 - acc: 0.7031 - val_loss: 0.6753 - val_acc: 0.6075\n",
      "Epoch 60/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5576 - acc: 0.690 - ETA: 0s - loss: 0.5488 - acc: 0.701 - ETA: 0s - loss: 0.5452 - acc: 0.704 - ETA: 0s - loss: 0.5442 - acc: 0.704 - ETA: 0s - loss: 0.5454 - acc: 0.703 - ETA: 0s - loss: 0.5464 - acc: 0.704 - ETA: 0s - loss: 0.5476 - acc: 0.703 - ETA: 0s - loss: 0.5490 - acc: 0.699 - 1s 82us/step - loss: 0.5482 - acc: 0.7006 - val_loss: 0.6775 - val_acc: 0.5967\n",
      "Epoch 61/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5426 - acc: 0.715 - ETA: 1s - loss: 0.5457 - acc: 0.708 - ETA: 0s - loss: 0.5494 - acc: 0.703 - ETA: 0s - loss: 0.5459 - acc: 0.703 - ETA: 0s - loss: 0.5456 - acc: 0.703 - ETA: 0s - loss: 0.5462 - acc: 0.703 - ETA: 0s - loss: 0.5465 - acc: 0.703 - ETA: 0s - loss: 0.5466 - acc: 0.702 - 2s 88us/step - loss: 0.5465 - acc: 0.7024 - val_loss: 0.6736 - val_acc: 0.6008\n",
      "Epoch 62/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5416 - acc: 0.714 - ETA: 1s - loss: 0.5439 - acc: 0.704 - ETA: 0s - loss: 0.5462 - acc: 0.703 - ETA: 0s - loss: 0.5441 - acc: 0.706 - ETA: 0s - loss: 0.5438 - acc: 0.706 - ETA: 0s - loss: 0.5450 - acc: 0.704 - ETA: 0s - loss: 0.5474 - acc: 0.702 - ETA: 0s - loss: 0.5468 - acc: 0.703 - 1s 85us/step - loss: 0.5478 - acc: 0.7022 - val_loss: 0.6743 - val_acc: 0.6023\n",
      "Epoch 63/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5350 - acc: 0.714 - ETA: 1s - loss: 0.5406 - acc: 0.708 - ETA: 0s - loss: 0.5386 - acc: 0.712 - ETA: 0s - loss: 0.5401 - acc: 0.712 - ETA: 0s - loss: 0.5427 - acc: 0.709 - ETA: 0s - loss: 0.5419 - acc: 0.709 - ETA: 0s - loss: 0.5426 - acc: 0.707 - ETA: 0s - loss: 0.5445 - acc: 0.705 - 2s 87us/step - loss: 0.5446 - acc: 0.7056 - val_loss: 0.6769 - val_acc: 0.5997\n",
      "Epoch 64/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5414 - acc: 0.705 - ETA: 1s - loss: 0.5430 - acc: 0.703 - ETA: 0s - loss: 0.5440 - acc: 0.705 - ETA: 0s - loss: 0.5450 - acc: 0.704 - ETA: 0s - loss: 0.5461 - acc: 0.703 - ETA: 0s - loss: 0.5441 - acc: 0.706 - ETA: 0s - loss: 0.5446 - acc: 0.704 - ETA: 0s - loss: 0.5444 - acc: 0.706 - 1s 84us/step - loss: 0.5446 - acc: 0.7061 - val_loss: 0.6763 - val_acc: 0.6034\n",
      "Epoch 65/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5369 - acc: 0.696 - ETA: 1s - loss: 0.5388 - acc: 0.702 - ETA: 0s - loss: 0.5401 - acc: 0.705 - ETA: 0s - loss: 0.5430 - acc: 0.704 - ETA: 0s - loss: 0.5407 - acc: 0.708 - ETA: 0s - loss: 0.5423 - acc: 0.706 - ETA: 0s - loss: 0.5441 - acc: 0.704 - ETA: 0s - loss: 0.5442 - acc: 0.704 - 2s 90us/step - loss: 0.5445 - acc: 0.7050 - val_loss: 0.6777 - val_acc: 0.5992\n",
      "Epoch 66/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5400 - acc: 0.718 - ETA: 1s - loss: 0.5380 - acc: 0.715 - ETA: 0s - loss: 0.5443 - acc: 0.703 - ETA: 0s - loss: 0.5431 - acc: 0.704 - ETA: 0s - loss: 0.5421 - acc: 0.706 - ETA: 0s - loss: 0.5416 - acc: 0.705 - ETA: 0s - loss: 0.5418 - acc: 0.705 - ETA: 0s - loss: 0.5421 - acc: 0.705 - 1s 83us/step - loss: 0.5422 - acc: 0.7060 - val_loss: 0.6800 - val_acc: 0.5982\n",
      "Epoch 67/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5354 - acc: 0.718 - ETA: 1s - loss: 0.5385 - acc: 0.714 - ETA: 0s - loss: 0.5364 - acc: 0.713 - ETA: 0s - loss: 0.5377 - acc: 0.713 - ETA: 0s - loss: 0.5397 - acc: 0.711 - ETA: 0s - loss: 0.5423 - acc: 0.707 - ETA: 0s - loss: 0.5430 - acc: 0.707 - ETA: 0s - loss: 0.5406 - acc: 0.711 - 1s 85us/step - loss: 0.5426 - acc: 0.7090 - val_loss: 0.6796 - val_acc: 0.5992\n",
      "Epoch 68/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5288 - acc: 0.704 - ETA: 0s - loss: 0.5428 - acc: 0.703 - ETA: 0s - loss: 0.5390 - acc: 0.708 - ETA: 0s - loss: 0.5362 - acc: 0.711 - ETA: 0s - loss: 0.5361 - acc: 0.712 - ETA: 0s - loss: 0.5383 - acc: 0.708 - ETA: 0s - loss: 0.5379 - acc: 0.708 - ETA: 0s - loss: 0.5405 - acc: 0.706 - 1s 84us/step - loss: 0.5404 - acc: 0.7065 - val_loss: 0.6805 - val_acc: 0.5995\n",
      "Epoch 69/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5316 - acc: 0.720 - ETA: 0s - loss: 0.5434 - acc: 0.709 - ETA: 0s - loss: 0.5424 - acc: 0.708 - ETA: 0s - loss: 0.5417 - acc: 0.709 - ETA: 0s - loss: 0.5424 - acc: 0.710 - ETA: 0s - loss: 0.5430 - acc: 0.708 - ETA: 0s - loss: 0.5427 - acc: 0.709 - ETA: 0s - loss: 0.5427 - acc: 0.708 - 2s 88us/step - loss: 0.5420 - acc: 0.7089 - val_loss: 0.6801 - val_acc: 0.5988\n",
      "Epoch 70/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5391 - acc: 0.712 - ETA: 0s - loss: 0.5407 - acc: 0.710 - ETA: 0s - loss: 0.5398 - acc: 0.706 - ETA: 0s - loss: 0.5354 - acc: 0.711 - ETA: 0s - loss: 0.5400 - acc: 0.706 - ETA: 0s - loss: 0.5413 - acc: 0.705 - ETA: 0s - loss: 0.5407 - acc: 0.707 - ETA: 0s - loss: 0.5399 - acc: 0.708 - 1s 87us/step - loss: 0.5395 - acc: 0.7092 - val_loss: 0.6879 - val_acc: 0.5993\n",
      "Epoch 71/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5335 - acc: 0.714 - ETA: 0s - loss: 0.5260 - acc: 0.716 - ETA: 0s - loss: 0.5319 - acc: 0.714 - ETA: 0s - loss: 0.5336 - acc: 0.713 - ETA: 0s - loss: 0.5334 - acc: 0.715 - ETA: 0s - loss: 0.5362 - acc: 0.712 - ETA: 0s - loss: 0.5385 - acc: 0.710 - ETA: 0s - loss: 0.5408 - acc: 0.708 - 1s 84us/step - loss: 0.5414 - acc: 0.7076 - val_loss: 0.6876 - val_acc: 0.5989\n",
      "Epoch 72/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5404 - acc: 0.715 - ETA: 0s - loss: 0.5379 - acc: 0.712 - ETA: 0s - loss: 0.5325 - acc: 0.715 - ETA: 0s - loss: 0.5386 - acc: 0.708 - ETA: 0s - loss: 0.5391 - acc: 0.708 - ETA: 0s - loss: 0.5384 - acc: 0.711 - ETA: 0s - loss: 0.5397 - acc: 0.710 - ETA: 0s - loss: 0.5387 - acc: 0.710 - 1s 85us/step - loss: 0.5398 - acc: 0.7090 - val_loss: 0.6860 - val_acc: 0.5966\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5536 - acc: 0.700 - ETA: 0s - loss: 0.5441 - acc: 0.707 - ETA: 0s - loss: 0.5340 - acc: 0.716 - ETA: 0s - loss: 0.5307 - acc: 0.717 - ETA: 0s - loss: 0.5366 - acc: 0.710 - ETA: 0s - loss: 0.5381 - acc: 0.709 - ETA: 0s - loss: 0.5385 - acc: 0.709 - ETA: 0s - loss: 0.5385 - acc: 0.711 - 1s 83us/step - loss: 0.5393 - acc: 0.7106 - val_loss: 0.6913 - val_acc: 0.5908\n",
      "Epoch 74/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5361 - acc: 0.715 - ETA: 0s - loss: 0.5388 - acc: 0.712 - ETA: 0s - loss: 0.5389 - acc: 0.709 - ETA: 0s - loss: 0.5387 - acc: 0.709 - ETA: 0s - loss: 0.5397 - acc: 0.706 - ETA: 0s - loss: 0.5391 - acc: 0.708 - ETA: 0s - loss: 0.5406 - acc: 0.708 - ETA: 0s - loss: 0.5401 - acc: 0.709 - 1s 85us/step - loss: 0.5407 - acc: 0.7088 - val_loss: 0.6815 - val_acc: 0.6011\n",
      "Epoch 75/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5297 - acc: 0.720 - ETA: 0s - loss: 0.5312 - acc: 0.719 - ETA: 0s - loss: 0.5282 - acc: 0.718 - ETA: 0s - loss: 0.5351 - acc: 0.712 - ETA: 0s - loss: 0.5357 - acc: 0.711 - ETA: 0s - loss: 0.5396 - acc: 0.708 - ETA: 0s - loss: 0.5399 - acc: 0.708 - ETA: 0s - loss: 0.5402 - acc: 0.708 - 1s 85us/step - loss: 0.5403 - acc: 0.7090 - val_loss: 0.6848 - val_acc: 0.5977\n",
      "Epoch 76/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5253 - acc: 0.719 - ETA: 1s - loss: 0.5271 - acc: 0.716 - ETA: 0s - loss: 0.5307 - acc: 0.715 - ETA: 0s - loss: 0.5339 - acc: 0.713 - ETA: 0s - loss: 0.5367 - acc: 0.711 - ETA: 0s - loss: 0.5374 - acc: 0.710 - ETA: 0s - loss: 0.5356 - acc: 0.712 - ETA: 0s - loss: 0.5342 - acc: 0.713 - 1s 86us/step - loss: 0.5354 - acc: 0.7132 - val_loss: 0.6836 - val_acc: 0.5969\n",
      "Epoch 77/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5232 - acc: 0.723 - ETA: 0s - loss: 0.5313 - acc: 0.715 - ETA: 0s - loss: 0.5313 - acc: 0.717 - ETA: 0s - loss: 0.5327 - acc: 0.715 - ETA: 0s - loss: 0.5342 - acc: 0.713 - ETA: 0s - loss: 0.5356 - acc: 0.713 - ETA: 0s - loss: 0.5354 - acc: 0.713 - ETA: 0s - loss: 0.5349 - acc: 0.714 - 1s 85us/step - loss: 0.5349 - acc: 0.7142 - val_loss: 0.6846 - val_acc: 0.6023\n",
      "Epoch 78/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5311 - acc: 0.724 - ETA: 1s - loss: 0.5315 - acc: 0.720 - ETA: 0s - loss: 0.5368 - acc: 0.718 - ETA: 0s - loss: 0.5342 - acc: 0.715 - ETA: 0s - loss: 0.5347 - acc: 0.715 - ETA: 0s - loss: 0.5343 - acc: 0.714 - ETA: 0s - loss: 0.5349 - acc: 0.713 - ETA: 0s - loss: 0.5357 - acc: 0.712 - 1s 87us/step - loss: 0.5355 - acc: 0.7125 - val_loss: 0.6873 - val_acc: 0.5988\n",
      "Epoch 79/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5397 - acc: 0.713 - ETA: 1s - loss: 0.5380 - acc: 0.710 - ETA: 0s - loss: 0.5373 - acc: 0.710 - ETA: 0s - loss: 0.5359 - acc: 0.713 - ETA: 0s - loss: 0.5350 - acc: 0.715 - ETA: 0s - loss: 0.5346 - acc: 0.716 - ETA: 0s - loss: 0.5339 - acc: 0.715 - ETA: 0s - loss: 0.5341 - acc: 0.714 - 1s 87us/step - loss: 0.5339 - acc: 0.7160 - val_loss: 0.6877 - val_acc: 0.5977\n",
      "Epoch 80/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5151 - acc: 0.732 - ETA: 0s - loss: 0.5152 - acc: 0.730 - ETA: 0s - loss: 0.5185 - acc: 0.725 - ETA: 0s - loss: 0.5272 - acc: 0.719 - ETA: 0s - loss: 0.5266 - acc: 0.719 - ETA: 0s - loss: 0.5305 - acc: 0.716 - ETA: 0s - loss: 0.5327 - acc: 0.714 - ETA: 0s - loss: 0.5341 - acc: 0.713 - 1s 83us/step - loss: 0.5340 - acc: 0.7129 - val_loss: 0.6899 - val_acc: 0.5989\n",
      "Epoch 81/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5281 - acc: 0.718 - ETA: 1s - loss: 0.5247 - acc: 0.722 - ETA: 0s - loss: 0.5224 - acc: 0.723 - ETA: 0s - loss: 0.5281 - acc: 0.721 - ETA: 0s - loss: 0.5327 - acc: 0.715 - ETA: 0s - loss: 0.5340 - acc: 0.713 - ETA: 0s - loss: 0.5355 - acc: 0.712 - ETA: 0s - loss: 0.5348 - acc: 0.712 - 2s 90us/step - loss: 0.5347 - acc: 0.7131 - val_loss: 0.6899 - val_acc: 0.5973\n",
      "Epoch 82/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5429 - acc: 0.710 - ETA: 0s - loss: 0.5354 - acc: 0.712 - ETA: 0s - loss: 0.5337 - acc: 0.708 - ETA: 0s - loss: 0.5313 - acc: 0.713 - ETA: 0s - loss: 0.5325 - acc: 0.712 - ETA: 0s - loss: 0.5330 - acc: 0.712 - ETA: 0s - loss: 0.5336 - acc: 0.711 - ETA: 0s - loss: 0.5341 - acc: 0.712 - 2s 93us/step - loss: 0.5345 - acc: 0.7121 - val_loss: 0.6888 - val_acc: 0.5953\n",
      "Epoch 83/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5082 - acc: 0.742 - ETA: 1s - loss: 0.5201 - acc: 0.733 - ETA: 1s - loss: 0.5241 - acc: 0.726 - ETA: 0s - loss: 0.5236 - acc: 0.725 - ETA: 0s - loss: 0.5257 - acc: 0.723 - ETA: 0s - loss: 0.5286 - acc: 0.721 - ETA: 0s - loss: 0.5310 - acc: 0.718 - ETA: 0s - loss: 0.5309 - acc: 0.717 - 2s 100us/step - loss: 0.5314 - acc: 0.7166 - val_loss: 0.6924 - val_acc: 0.6046\n",
      "Epoch 84/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5381 - acc: 0.703 - ETA: 1s - loss: 0.5266 - acc: 0.714 - ETA: 1s - loss: 0.5267 - acc: 0.717 - ETA: 0s - loss: 0.5313 - acc: 0.716 - ETA: 0s - loss: 0.5336 - acc: 0.715 - ETA: 0s - loss: 0.5312 - acc: 0.717 - ETA: 0s - loss: 0.5324 - acc: 0.716 - ETA: 0s - loss: 0.5340 - acc: 0.714 - 2s 97us/step - loss: 0.5348 - acc: 0.7132 - val_loss: 0.6966 - val_acc: 0.5970\n",
      "Epoch 85/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5243 - acc: 0.724 - ETA: 1s - loss: 0.5301 - acc: 0.721 - ETA: 0s - loss: 0.5289 - acc: 0.720 - ETA: 0s - loss: 0.5290 - acc: 0.720 - ETA: 0s - loss: 0.5311 - acc: 0.719 - ETA: 0s - loss: 0.5339 - acc: 0.717 - ETA: 0s - loss: 0.5355 - acc: 0.714 - ETA: 0s - loss: 0.5349 - acc: 0.714 - 2s 88us/step - loss: 0.5343 - acc: 0.7158 - val_loss: 0.6932 - val_acc: 0.5947\n",
      "Epoch 86/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5270 - acc: 0.718 - ETA: 0s - loss: 0.5259 - acc: 0.718 - ETA: 0s - loss: 0.5292 - acc: 0.716 - ETA: 0s - loss: 0.5277 - acc: 0.716 - ETA: 0s - loss: 0.5291 - acc: 0.716 - ETA: 0s - loss: 0.5322 - acc: 0.714 - ETA: 0s - loss: 0.5321 - acc: 0.714 - ETA: 0s - loss: 0.5323 - acc: 0.714 - 1s 86us/step - loss: 0.5331 - acc: 0.7138 - val_loss: 0.6993 - val_acc: 0.5962\n",
      "Epoch 87/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5396 - acc: 0.710 - ETA: 1s - loss: 0.5340 - acc: 0.711 - ETA: 0s - loss: 0.5361 - acc: 0.710 - ETA: 0s - loss: 0.5302 - acc: 0.716 - ETA: 0s - loss: 0.5324 - acc: 0.715 - ETA: 0s - loss: 0.5318 - acc: 0.717 - ETA: 0s - loss: 0.5316 - acc: 0.717 - ETA: 0s - loss: 0.5313 - acc: 0.717 - 2s 88us/step - loss: 0.5313 - acc: 0.7178 - val_loss: 0.6936 - val_acc: 0.5982\n",
      "Epoch 88/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5180 - acc: 0.723 - ETA: 1s - loss: 0.5191 - acc: 0.723 - ETA: 0s - loss: 0.5187 - acc: 0.724 - ETA: 0s - loss: 0.5251 - acc: 0.716 - ETA: 0s - loss: 0.5246 - acc: 0.718 - ETA: 0s - loss: 0.5251 - acc: 0.716 - ETA: 0s - loss: 0.5274 - acc: 0.715 - ETA: 0s - loss: 0.5292 - acc: 0.714 - 2s 89us/step - loss: 0.5297 - acc: 0.7145 - val_loss: 0.6936 - val_acc: 0.5991\n",
      "Epoch 89/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5354 - acc: 0.715 - ETA: 1s - loss: 0.5280 - acc: 0.719 - ETA: 0s - loss: 0.5297 - acc: 0.719 - ETA: 0s - loss: 0.5328 - acc: 0.717 - ETA: 0s - loss: 0.5323 - acc: 0.716 - ETA: 0s - loss: 0.5323 - acc: 0.717 - ETA: 0s - loss: 0.5296 - acc: 0.719 - ETA: 0s - loss: 0.5297 - acc: 0.720 - 2s 88us/step - loss: 0.5295 - acc: 0.7209 - val_loss: 0.6959 - val_acc: 0.5954\n",
      "Epoch 90/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5246 - acc: 0.722 - ETA: 0s - loss: 0.5272 - acc: 0.722 - ETA: 0s - loss: 0.5290 - acc: 0.717 - ETA: 0s - loss: 0.5322 - acc: 0.715 - ETA: 0s - loss: 0.5333 - acc: 0.715 - ETA: 0s - loss: 0.5316 - acc: 0.718 - ETA: 0s - loss: 0.5316 - acc: 0.716 - ETA: 0s - loss: 0.5314 - acc: 0.715 - 1s 84us/step - loss: 0.5301 - acc: 0.7170 - val_loss: 0.6951 - val_acc: 0.6014\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5365 - acc: 0.714 - ETA: 0s - loss: 0.5321 - acc: 0.713 - ETA: 0s - loss: 0.5252 - acc: 0.719 - ETA: 0s - loss: 0.5257 - acc: 0.721 - ETA: 0s - loss: 0.5281 - acc: 0.716 - ETA: 0s - loss: 0.5282 - acc: 0.716 - ETA: 0s - loss: 0.5281 - acc: 0.719 - ETA: 0s - loss: 0.5288 - acc: 0.717 - 2s 88us/step - loss: 0.5292 - acc: 0.7173 - val_loss: 0.6985 - val_acc: 0.5919\n",
      "Epoch 92/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5209 - acc: 0.724 - ETA: 1s - loss: 0.5202 - acc: 0.728 - ETA: 0s - loss: 0.5233 - acc: 0.722 - ETA: 0s - loss: 0.5273 - acc: 0.718 - ETA: 0s - loss: 0.5296 - acc: 0.718 - ETA: 0s - loss: 0.5292 - acc: 0.717 - ETA: 0s - loss: 0.5283 - acc: 0.718 - ETA: 0s - loss: 0.5281 - acc: 0.718 - 2s 89us/step - loss: 0.5288 - acc: 0.7182 - val_loss: 0.6976 - val_acc: 0.6026\n",
      "Epoch 93/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5237 - acc: 0.722 - ETA: 1s - loss: 0.5370 - acc: 0.715 - ETA: 0s - loss: 0.5354 - acc: 0.715 - ETA: 0s - loss: 0.5360 - acc: 0.710 - ETA: 0s - loss: 0.5325 - acc: 0.712 - ETA: 0s - loss: 0.5312 - acc: 0.716 - ETA: 0s - loss: 0.5310 - acc: 0.716 - ETA: 0s - loss: 0.5309 - acc: 0.715 - 2s 88us/step - loss: 0.5308 - acc: 0.7160 - val_loss: 0.7029 - val_acc: 0.6054\n",
      "Epoch 94/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5219 - acc: 0.730 - ETA: 1s - loss: 0.5214 - acc: 0.728 - ETA: 0s - loss: 0.5181 - acc: 0.727 - ETA: 0s - loss: 0.5202 - acc: 0.725 - ETA: 0s - loss: 0.5215 - acc: 0.724 - ETA: 0s - loss: 0.5216 - acc: 0.723 - ETA: 0s - loss: 0.5256 - acc: 0.719 - ETA: 0s - loss: 0.5259 - acc: 0.718 - 2s 90us/step - loss: 0.5278 - acc: 0.7169 - val_loss: 0.7006 - val_acc: 0.5921\n",
      "Epoch 95/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5200 - acc: 0.721 - ETA: 1s - loss: 0.5195 - acc: 0.725 - ETA: 0s - loss: 0.5196 - acc: 0.725 - ETA: 0s - loss: 0.5235 - acc: 0.724 - ETA: 0s - loss: 0.5248 - acc: 0.721 - ETA: 0s - loss: 0.5246 - acc: 0.721 - ETA: 0s - loss: 0.5264 - acc: 0.718 - ETA: 0s - loss: 0.5283 - acc: 0.716 - 2s 89us/step - loss: 0.5286 - acc: 0.7180 - val_loss: 0.7001 - val_acc: 0.6008\n",
      "Epoch 96/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5264 - acc: 0.725 - ETA: 0s - loss: 0.5196 - acc: 0.723 - ETA: 0s - loss: 0.5148 - acc: 0.725 - ETA: 0s - loss: 0.5188 - acc: 0.724 - ETA: 0s - loss: 0.5199 - acc: 0.725 - ETA: 0s - loss: 0.5215 - acc: 0.723 - ETA: 0s - loss: 0.5221 - acc: 0.723 - ETA: 0s - loss: 0.5246 - acc: 0.721 - 2s 88us/step - loss: 0.5269 - acc: 0.7186 - val_loss: 0.6986 - val_acc: 0.5981\n",
      "Epoch 97/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5116 - acc: 0.732 - ETA: 1s - loss: 0.5219 - acc: 0.721 - ETA: 0s - loss: 0.5214 - acc: 0.724 - ETA: 0s - loss: 0.5176 - acc: 0.727 - ETA: 0s - loss: 0.5226 - acc: 0.723 - ETA: 0s - loss: 0.5267 - acc: 0.718 - ETA: 0s - loss: 0.5266 - acc: 0.719 - ETA: 0s - loss: 0.5272 - acc: 0.719 - 2s 88us/step - loss: 0.5265 - acc: 0.7199 - val_loss: 0.7029 - val_acc: 0.5935\n",
      "Epoch 98/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5251 - acc: 0.715 - ETA: 1s - loss: 0.5197 - acc: 0.725 - ETA: 0s - loss: 0.5206 - acc: 0.724 - ETA: 0s - loss: 0.5258 - acc: 0.720 - ETA: 0s - loss: 0.5269 - acc: 0.719 - ETA: 0s - loss: 0.5296 - acc: 0.716 - ETA: 0s - loss: 0.5291 - acc: 0.718 - ETA: 0s - loss: 0.5266 - acc: 0.721 - 2s 88us/step - loss: 0.5275 - acc: 0.7198 - val_loss: 0.7018 - val_acc: 0.5974\n",
      "Epoch 99/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5269 - acc: 0.730 - ETA: 1s - loss: 0.5208 - acc: 0.729 - ETA: 0s - loss: 0.5232 - acc: 0.724 - ETA: 0s - loss: 0.5254 - acc: 0.722 - ETA: 0s - loss: 0.5247 - acc: 0.722 - ETA: 0s - loss: 0.5251 - acc: 0.722 - ETA: 0s - loss: 0.5241 - acc: 0.723 - ETA: 0s - loss: 0.5256 - acc: 0.722 - 1s 87us/step - loss: 0.5256 - acc: 0.7211 - val_loss: 0.7071 - val_acc: 0.5905\n",
      "Epoch 100/100\n",
      "17219/17219 [==============================] - ETA: 1s - loss: 0.5278 - acc: 0.720 - ETA: 1s - loss: 0.5226 - acc: 0.722 - ETA: 0s - loss: 0.5278 - acc: 0.717 - ETA: 0s - loss: 0.5268 - acc: 0.717 - ETA: 0s - loss: 0.5274 - acc: 0.716 - ETA: 0s - loss: 0.5286 - acc: 0.714 - ETA: 0s - loss: 0.5277 - acc: 0.715 - ETA: 0s - loss: 0.5280 - acc: 0.715 - 2s 87us/step - loss: 0.5275 - acc: 0.7159 - val_loss: 0.7036 - val_acc: 0.5995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b07710>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_st = Sequential()\n",
    "model_st.add(Conv2D(64, kernel_size=2, activation='relu', input_shape=(5,20,1)))\n",
    "model_st.add(Flatten())\n",
    "model_st.add(Dense(units=32, activation='relu'))\n",
    "model_st.add(Dense(2, activation='softmax'))\n",
    "model_st.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "model_st.fit(X_train_train_small_np, y_train_train_small_np,\n",
    "                 validation_data=(X_vali_small_np, y_vali_small_np),\n",
    "                 epochs=100, batch_size=2048, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
