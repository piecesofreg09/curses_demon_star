{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All window size is based on `[44, 80]`, 44 is the height, 80 is the width.** This setting is set in the file `game.py` on line 33, but it is not recommended to change the size. It will result in the failure of prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import all the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle, os, math\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data from the data folder. This step only uses the data for surviving in the rain of topedoes for the fighter. There are two types of data.\n",
    "\n",
    "1. with no fires from the fighter,\n",
    "2. with fires from the fighter.\n",
    "\n",
    "The difference lies in the number of data points in each class. With fires from the fighter, the number of possible movements are higher than the case without fires from the fighter. Also the total number of data points is less than the second case, because the lives are consumed faster. As can be seen, the first dataset has 154k data points, the second dataset has 200k data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154332, 24)\n",
      "Wall time: 58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_dir = os.path.join(os.curdir, 'Data', '200000_sur_nofires', 'basic_data_pics.pkl')\n",
    "with open(data_dir, 'rb') as in_file:\n",
    "    ot = pickle.load(in_file)\n",
    "data_pics_nofires = ot['data']\n",
    "target_pics_nofires = ot['target']\n",
    "print(data_pics_nofires.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200036, 24)\n",
      "Wall time: 63 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_dir = os.path.join(os.curdir, 'Data', '200000_sur_fires', 'basic_data_pics.pkl')\n",
    "with open(data_dir, 'rb') as in_file:\n",
    "    ot = pickle.load(in_file)\n",
    "data_pics_fires = ot['data']\n",
    "target_pics_fires = ot['target']\n",
    "print(data_pics_fires.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess First Dataset (without fires)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first data is unbalanced with classes. The ratio of classes 0 and 1 are almost 1:5. Very unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 has 23761 points\n",
      "class 1 has 115137 points\n"
     ]
    }
   ],
   "source": [
    "X_train_nf, X_test_nf, y_train_nf, y_test_nf = train_test_split(\n",
    "    data_pics_nofires, target_pics_nofires, test_size=0.1, random_state=152)\n",
    "X_train.shape\n",
    "X_train_train_nf, X_vali_nf, y_train_train_nf, y_vali_nf = train_test_split(\n",
    "    X_train_nf, y_train_nf, test_size=0.3, random_state=15545)\n",
    "print('class 0 has ' + str(len(y_train_nf.index[y_train_nf[0] == 0].tolist())) + ' points')\n",
    "print('class 1 has ' + str(len(y_train_nf.index[y_train_nf[0] == 1].tolist())) + ' points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23761 + 23761 = 47522\n"
     ]
    }
   ],
   "source": [
    "index_0 = y_train_nf.index[y_train_nf[0] == 0].tolist()\n",
    "index_1 = y_train_nf.index[y_train_nf[0] != 0].tolist()\n",
    "index_1_comparable_to_0 = np.random.choice(index_1, math.floor(len(index_0) * 1))\n",
    "samples_nf = np.concatenate([index_0, index_1_comparable_to_0])\n",
    "print(str(len(index_0)) + ' + ' + str(len(index_1_comparable_to_0)) + ' = ' + str(len(samples_nf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`small_data_nf` and `small_target_nf` are the balanced data. All variables with `_small` is the balanced result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data_nf = data_pics_nofires.iloc[samples_nf, :]\n",
    "small_target_nf = target_pics_nofires.iloc[samples_nf, :]\n",
    "\n",
    "X_train_small_nf, X_test_small_nf, y_train_small_nf, y_test_small_nf = train_test_split(\n",
    "    small_data_nf, small_target_nf, test_size=0.3, random_state=1152)\n",
    "X_train_train_small_nf, X_vali_small_nf, y_train_train_small_nf, y_vali_small_nf = train_test_split(\n",
    "    X_train_small_nf, y_train_small_nf, test_size=0.3, random_state=8155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess Second Dataset (with fires)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second data is unbalanced with classes. The ratio of classes 0 and 1 are almost 1:10. More unbalanced compared to the second dataset. It is because of the fires helps with the possible movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 has 15101 points\n",
      "class 1 has 164931 points\n"
     ]
    }
   ],
   "source": [
    "X_train_fs, X_test_fs, y_train_fs, y_test_fs = train_test_split(\n",
    "    data_pics_fires, target_pics_fires, test_size=0.1, random_state=218)\n",
    "X_train_fs.shape\n",
    "X_train_train_fs, X_vali_fs, y_train_train_fs, y_vali_fs = train_test_split(\n",
    "    X_train_fs, y_train_fs, test_size=0.3, random_state=1515615)\n",
    "print('class 0 has ' + str(len(y_train_fs.index[y_train_fs[0] == 0].tolist())) + ' points')\n",
    "print('class 1 has ' + str(len(y_train_fs.index[y_train_fs[0] == 1].tolist())) + ' points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create balanced classes sample data, each class has 15061 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15101 + 15101 = 30202\n"
     ]
    }
   ],
   "source": [
    "index_0 = y_train_fs.index[y_train_fs[0] == 0].tolist()\n",
    "index_1 = y_train_fs.index[y_train_fs[0] != 0].tolist()\n",
    "index_1_comparable_to_0 = np.random.choice(index_1, math.floor(len(index_0) * 1))\n",
    "samples_fs = np.concatenate([index_0, index_1_comparable_to_0])\n",
    "print(str(len(index_0)) + ' + ' + str(len(index_1_comparable_to_0)) + ' = ' + str(len(samples_fs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`small_data_fs` and `small_target_fs` are the balanced data. All variables with `_small` is the balanced result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data_fs = data_pics_fires.iloc[samples_fs, :]\n",
    "small_target_fs = target_pics_fires.iloc[samples_fs, :]\n",
    "\n",
    "X_train_small_fs, X_test_small_fs, y_train_small_fs, y_test_small_fs = train_test_split(\n",
    "    small_data_fs, small_target_fs, test_size=0.3, random_state=152)\n",
    "X_train_train_small_fs, X_vali_small_fs, y_train_train_small_fs, y_vali_small_fs = train_test_split(\n",
    "    X_train_small_fs, y_train_small_fs, test_size=0.3, random_state=152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pretraining Several Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scikit models, target (y) has to be ravelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_small_nf_m = np.ravel(y_train_small_nf)\n",
    "y_test_small_nf_m = np.ravel(y_test_small_nf)\n",
    "y_train_train_small_nf_m = np.ravel(y_train_train_small_nf)\n",
    "y_vali_small_nf_m = np.ravel(y_vali_small_nf)\n",
    "y_train_nf_m = np.ravel(y_train_nf)\n",
    "y_test_nf_m = np.ravel(y_test_nf)\n",
    "y_train_train_nf_m = np.ravel(y_train_train_nf)\n",
    "y_vali_nf_m = np.ravel(y_vali_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_small_fs_m = np.ravel(y_train_small_fs)\n",
    "y_test_small_fs_m = np.ravel(y_test_small_fs)\n",
    "y_train_train_small_fs_m = np.ravel(y_train_train_small_fs)\n",
    "y_vali_small_fs_m = np.ravel(y_vali_small_fs)\n",
    "y_train_fs_m = np.ravel(y_train_fs)\n",
    "y_test_fs_m = np.ravel(y_test_fs)\n",
    "y_train_train_fs_m = np.ravel(y_train_train_fs)\n",
    "y_vali_fs_m = np.ravel(y_vali_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 SVC with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **no fires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.945991442800028"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nf = SVC(C=10.0, gamma='auto', verbose=True)\n",
    "clf_nf.fit(X_train_small_nf, y_train_small_nf_m)\n",
    "clf_nf.score(X_test_small_nf, y_test_small_nf_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.94048693, 0.94642857, 0.94678932, 0.94660895, 0.95147908,\n",
       "       0.93921356])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf_nf, X_train_small_nf, y_train_small_nf_m, cv=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ** With fires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9381966670345436"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_fs = SVC(C=10.0, gamma='auto', verbose=True)\n",
    "clf_fs.fit(X_train_small_fs, y_train_small_fs_m)\n",
    "clf_fs.score(X_test_small_fs, y_test_small_fs_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.93728717, 0.93955732, 0.93586833, 0.93897247, 0.93698552,\n",
       "       0.94010786])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf_fs, X_train_small_fs, y_train_small_fs_m, cv=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of different datasets are almost the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 MLPC with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **no fires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7848074630006313"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc_nf = MLPClassifier(hidden_layer_sizes=(50, 20),\n",
    "                        alpha=0.15, max_iter=1000, batch_size=5000,\n",
    "                        verbose=False, learning_rate_init=0.01, tol=1e-5,\n",
    "                        learning_rate='adaptive')\n",
    "\n",
    "mlpc_nf.fit(X_train_small_nf, y_train_small_nf_m)\n",
    "mlpc_nf.score(X_test_small_nf, y_test_small_nf_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87571386, 0.89493462, 0.78505937, 0.82684503, 0.86410102])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mlpc_nf, X_train_small_nf, y_train_small_nf_m, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ** With fires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.887098554243461"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc_fs = MLPClassifier(hidden_layer_sizes=(50, 20),\n",
    "                        alpha=0.15, max_iter=1000, batch_size=5000,\n",
    "                        verbose=False, learning_rate_init=0.01, tol=1e-5,\n",
    "                        learning_rate='adaptive')\n",
    "\n",
    "mlpc_1.fit(X_train_small_fs, y_train_small_fs_m)\n",
    "mlpc_1.score(X_test_small_fs, y_test_small_fs_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88507921, 0.88129581, 0.80368969, 0.84531693, 0.84362432])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mlpc_fs, X_train_small_fs, y_train_small_fs_m, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 NN with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **no fires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23285 samples, validate on 9980 samples\n",
      "Epoch 1/20\n",
      "23285/23285 [==============================] - 0s 14us/step - loss: 0.2685 - acc: 0.5793 - val_loss: 0.2169 - val_acc: 0.6623\n",
      "Epoch 2/20\n",
      "23285/23285 [==============================] - 0s 2us/step - loss: 0.2083 - acc: 0.6774 - val_loss: 0.2003 - val_acc: 0.6974\n",
      "Epoch 3/20\n",
      "23285/23285 [==============================] - 0s 2us/step - loss: 0.1948 - acc: 0.7138 - val_loss: 0.1965 - val_acc: 0.7007\n",
      "Epoch 4/20\n",
      "23285/23285 [==============================] - 0s 2us/step - loss: 0.1910 - acc: 0.7191 - val_loss: 0.1922 - val_acc: 0.7134\n",
      "Epoch 5/20\n",
      "23285/23285 [==============================] - 0s 2us/step - loss: 0.1879 - acc: 0.7279 - val_loss: 0.1897 - val_acc: 0.7251\n",
      "Epoch 6/20\n",
      "23285/23285 [==============================] - 0s 2us/step - loss: 0.1827 - acc: 0.7394 - val_loss: 0.1857 - val_acc: 0.7204\n",
      "Epoch 7/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1805 - acc: 0.7397 - val_loss: 0.1813 - val_acc: 0.7326\n",
      "Epoch 8/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1784 - acc: 0.7422 - val_loss: 0.1782 - val_acc: 0.7419\n",
      "Epoch 9/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1767 - acc: 0.7489 - val_loss: 0.1794 - val_acc: 0.7333\n",
      "Epoch 10/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1753 - acc: 0.7487 - val_loss: 0.1783 - val_acc: 0.7347\n",
      "Epoch 11/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1732 - acc: 0.7521 - val_loss: 0.1725 - val_acc: 0.7548\n",
      "Epoch 12/20\n",
      "23285/23285 [==============================] - 0s 2us/step - loss: 0.1706 - acc: 0.7595 - val_loss: 0.1766 - val_acc: 0.7373\n",
      "Epoch 13/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1711 - acc: 0.7527 - val_loss: 0.1701 - val_acc: 0.7603\n",
      "Epoch 14/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1682 - acc: 0.7627 - val_loss: 0.1696 - val_acc: 0.7541\n",
      "Epoch 15/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1664 - acc: 0.7642 - val_loss: 0.1670 - val_acc: 0.7631\n",
      "Epoch 16/20\n",
      "23285/23285 [==============================] - 0s 2us/step - loss: 0.1666 - acc: 0.7654 - val_loss: 0.1663 - val_acc: 0.7636\n",
      "Epoch 17/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1644 - acc: 0.7663 - val_loss: 0.1651 - val_acc: 0.7634\n",
      "Epoch 18/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1617 - acc: 0.7720 - val_loss: 0.1650 - val_acc: 0.7624\n",
      "Epoch 19/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1624 - acc: 0.7691 - val_loss: 0.1658 - val_acc: 0.7625\n",
      "Epoch 20/20\n",
      "23285/23285 [==============================] - 0s 1us/step - loss: 0.1594 - acc: 0.7757 - val_loss: 0.1605 - val_acc: 0.7691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x47c43a90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NNK_nf = Sequential()\n",
    "model_NNK_nf.add(Dense(units=12, activation='relu', input_dim=24))\n",
    "model_NNK_nf.add(Dense(units=6, activation='relu'))\n",
    "model_NNK_nf.add(Dense(units=1, activation='sigmoid'))\n",
    "model_NNK_nf.compile(loss='mean_squared_error',\n",
    "                     optimizer='rmsprop',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "model_NNK_nf.fit(X_train_train_small_nf, y_train_train_small_nf,\n",
    "                 validation_data=(X_vali_small_nf, y_vali_small_nf),\n",
    "                 epochs=20, batch_size=4096, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14257/14257 [==============================] - 0s 5us/step\n",
      "[0.15824912719968376, 0.7735147647355309]\n",
      "0.4994975398598057\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model_NNK_nf.evaluate(X_test_small_nf, y_test_small_nf, batch_size=128)\n",
    "print(loss_and_metrics)\n",
    "y_predict_nf = model_NNK_nf.predict(X_test_small_nf, batch_size=None, verbose=0)\n",
    "print(np.sum(y_predict_nf) / len(y_predict_nf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ** With fires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14798 samples, validate on 6343 samples\n",
      "Epoch 1/20\n",
      "14798/14798 [==============================] - 0s 18us/step - loss: 0.5005 - acc: 0.4980 - val_loss: 0.4819 - val_acc: 0.5067\n",
      "Epoch 2/20\n",
      "14798/14798 [==============================] - 0s 2us/step - loss: 0.4496 - acc: 0.4978 - val_loss: 0.3501 - val_acc: 0.5059\n",
      "Epoch 3/20\n",
      "14798/14798 [==============================] - 0s 2us/step - loss: 0.3248 - acc: 0.4926 - val_loss: 0.2662 - val_acc: 0.4674\n",
      "Epoch 4/20\n",
      "14798/14798 [==============================] - 0s 2us/step - loss: 0.2471 - acc: 0.4574 - val_loss: 0.2083 - val_acc: 0.6852\n",
      "Epoch 5/20\n",
      "14798/14798 [==============================] - 0s 2us/step - loss: 0.2054 - acc: 0.6980 - val_loss: 0.1986 - val_acc: 0.7369\n",
      "Epoch 6/20\n",
      "14798/14798 [==============================] - 0s 2us/step - loss: 0.2002 - acc: 0.7494 - val_loss: 0.1956 - val_acc: 0.6826\n",
      "Epoch 7/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1969 - acc: 0.7524 - val_loss: 0.1929 - val_acc: 0.7495\n",
      "Epoch 8/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1944 - acc: 0.7831 - val_loss: 0.1899 - val_acc: 0.8116\n",
      "Epoch 9/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1902 - acc: 0.7979 - val_loss: 0.1878 - val_acc: 0.8116\n",
      "Epoch 10/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1887 - acc: 0.8015 - val_loss: 0.1839 - val_acc: 0.8135\n",
      "Epoch 11/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1857 - acc: 0.7977 - val_loss: 0.1825 - val_acc: 0.8102\n",
      "Epoch 12/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1842 - acc: 0.8010 - val_loss: 0.1829 - val_acc: 0.7528\n",
      "Epoch 13/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1850 - acc: 0.7871 - val_loss: 0.1807 - val_acc: 0.8178\n",
      "Epoch 14/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1816 - acc: 0.8218 - val_loss: 0.1808 - val_acc: 0.7569\n",
      "Epoch 15/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1822 - acc: 0.8053 - val_loss: 0.1796 - val_acc: 0.8256\n",
      "Epoch 16/20\n",
      "14798/14798 [==============================] - 0s 8us/step - loss: 0.1797 - acc: 0.8252 - val_loss: 0.1773 - val_acc: 0.8154\n",
      "Epoch 17/20\n",
      "14798/14798 [==============================] - 0s 2us/step - loss: 0.1785 - acc: 0.8255 - val_loss: 0.1767 - val_acc: 0.8253\n",
      "Epoch 18/20\n",
      "14798/14798 [==============================] - 0s 2us/step - loss: 0.1772 - acc: 0.8267 - val_loss: 0.1753 - val_acc: 0.8286\n",
      "Epoch 19/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1758 - acc: 0.8311 - val_loss: 0.1768 - val_acc: 0.7643\n",
      "Epoch 20/20\n",
      "14798/14798 [==============================] - 0s 1us/step - loss: 0.1778 - acc: 0.8144 - val_loss: 0.1736 - val_acc: 0.8345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x47f2ae48>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NNK_fs = Sequential()\n",
    "model_NNK_fs.add(Dense(units=12, activation='relu', input_dim=24))\n",
    "model_NNK_fs.add(Dense(units=6, activation='relu'))\n",
    "model_NNK_fs.add(Dense(units=1, activation='sigmoid'))\n",
    "model_NNK_fs.compile(loss='mean_squared_error',\n",
    "                     optimizer='rmsprop',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "model_NNK_fs.fit(X_train_train_small_fs, y_train_train_small_fs,\n",
    "          validation_data=(X_vali_small_fs, y_vali_small_fs), \n",
    "          epochs=20, batch_size=4096, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9061/9061 [==============================] - 0s 7us/step\n",
      "[0.1744909577417842, 0.8367729828447422]\n",
      "0.3570299997930692\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model_NNK_fs.evaluate(X_test_small_fs, y_test_small_fs, batch_size=128)\n",
    "print(loss_and_metrics)\n",
    "y_predict_fs = model_NNK_fs.predict(X_test_small_fs, batch_size=None, verbose=0)\n",
    "print(np.sum(y_predict_fs) / len(y_predict_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Selecting models and Saving Models\n",
    "\n",
    "the SVC model apparently has a higher accuracy, grid search is used to find the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **no fires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=6, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf_nf_2 = SVC(gamma='auto', verbose=True)\n",
    "svc_vc_nf = GridSearchCV(clf_nf_2, parameters, cv=6, refit=True)\n",
    "svc_vc_nf.fit(X_train_small_nf, y_train_small_nf_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([ 20.33870033,  20.1581823 ,  20.92692582,  47.23078652,\n",
      "        37.32616735, 250.03938635]), 'std_fit_time': array([ 1.89203288,  0.94841152,  0.19896328,  4.2314855 ,  3.99647915,\n",
      "       31.23958638]), 'mean_score_time': array([2.23639031, 1.3252991 , 1.73850719, 1.24541525, 1.37714493,\n",
      "       1.3074249 ]), 'std_score_time': array([0.14395799, 0.13146583, 0.09331472, 0.08438314, 0.09505282,\n",
      "       0.11493295]), 'param_C': masked_array(data=[0.1, 0.1, 1, 1, 10, 10],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['rbf', 'linear', 'rbf', 'linear', 'rbf', 'linear'],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.1, 'kernel': 'rbf'}, {'C': 0.1, 'kernel': 'linear'}, {'C': 1, 'kernel': 'rbf'}, {'C': 1, 'kernel': 'linear'}, {'C': 10, 'kernel': 'rbf'}, {'C': 10, 'kernel': 'linear'}], 'split0_test_score': array([0.90333634, 0.85915239, 0.93417493, 0.85951307, 0.94048693,\n",
      "       0.85897205]), 'split1_test_score': array([0.9018759 , 0.85497835, 0.93668831, 0.85371573, 0.94642857,\n",
      "       0.85569986]), 'split2_test_score': array([0.90205628, 0.85515873, 0.93813131, 0.85497835, 0.94678932,\n",
      "       0.85551948]), 'split3_test_score': array([0.89664502, 0.8546176 , 0.93506494, 0.85407648, 0.94660895,\n",
      "       0.85443723]), 'split4_test_score': array([0.90584416, 0.85443723, 0.94011544, 0.85551948, 0.95147908,\n",
      "       0.8546176 ]), 'split5_test_score': array([0.89664502, 0.85263348, 0.93272006, 0.85299423, 0.93921356,\n",
      "       0.8517316 ]), 'mean_test_score': array([0.90106719, 0.85516308, 0.93614911, 0.85513302, 0.94516759,\n",
      "       0.85516308]), 'std_test_score': array([0.00338405, 0.00196529, 0.00247597, 0.00212368, 0.00415316,\n",
      "       0.00214183]), 'rank_test_score': array([3, 4, 2, 6, 1, 4]), 'split0_train_score': array([0.9027417 , 0.85541126, 0.94733045, 0.85638528, 0.96515152,\n",
      "       0.85559163]), 'split1_train_score': array([0.90314202, 0.85346849, 0.94653873, 0.85220591, 0.96403449,\n",
      "       0.85310775]), 'split2_train_score': array([0.90350276, 0.85617402, 0.94650265, 0.85581328, 0.96486418,\n",
      "       0.85646261]), 'split3_train_score': array([0.90404387, 0.85639046, 0.94592547, 0.85617402, 0.96450345,\n",
      "       0.85657083]), 'split4_train_score': array([0.90162693, 0.85458678, 0.94635836, 0.8553804 , 0.96378197,\n",
      "       0.85462285]), 'split5_train_score': array([0.90375528, 0.85631831, 0.9458894 , 0.85696764, 0.96515277,\n",
      "       0.85577721]), 'mean_train_score': array([0.90313543, 0.85539155, 0.94642417, 0.85548776, 0.9645814 ,\n",
      "       0.85535548]), 'std_train_score': array([0.00079321, 0.00106577, 0.00047952, 0.00154675, 0.00052848,\n",
      "       0.00119202])}\n"
     ]
    }
   ],
   "source": [
    "print(svc_vc_nf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.945991442800028"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_vc_nf.score(X_test_small_nf, y_test_small_nf_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "svc_best_nf = svc_vc_nf.best_estimator_\n",
    "joblib.dump(svc_best_nf, os.path.join(os.curdir,'Models', 'model_svc_survive_nf.joblib'))\n",
    "svc_best_loaded_nf = joblib.load(os.path.join(os.curdir,'Models', 'model_svc_survive_nf.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svc_best_loaded.predict([X_test_small.iloc[0, :]])\n",
    "svc_best_loaded_nf.predict([[i for i in range(24)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ** With fires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=6, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf_fs_2 = SVC(gamma='auto', verbose=True)\n",
    "svc_vc_fs = GridSearchCV(clf_fs_2, parameters, cv=6, refit=True)\n",
    "svc_vc_fs.fit(X_train_small_fs, y_train_small_fs_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([ 8.00113336,  7.7794445 ,  7.87745432, 15.08317482, 10.09467606,\n",
      "       72.6587652 ]), 'std_fit_time': array([0.39169842, 0.32830542, 0.13949482, 0.41731323, 0.5491174 ,\n",
      "       5.80136382]), 'mean_score_time': array([0.78524526, 0.38637201, 0.64806481, 0.38803879, 0.54238756,\n",
      "       0.47571421]), 'std_score_time': array([0.03032576, 0.01874297, 0.0480326 , 0.02326182, 0.04925825,\n",
      "       0.03367164]), 'param_C': masked_array(data=[0.1, 0.1, 1, 1, 10, 10],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['rbf', 'linear', 'rbf', 'linear', 'rbf', 'linear'],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.1, 'kernel': 'rbf'}, {'C': 0.1, 'kernel': 'linear'}, {'C': 1, 'kernel': 'rbf'}, {'C': 1, 'kernel': 'linear'}, {'C': 10, 'kernel': 'rbf'}, {'C': 10, 'kernel': 'linear'}], 'split0_test_score': array([0.91203178, 0.88337117, 0.93359818, 0.88337117, 0.93728717,\n",
      "       0.88337117]), 'split1_test_score': array([0.90011351, 0.8776958 , 0.93274688, 0.8776958 , 0.93955732,\n",
      "       0.8776958 ]), 'split2_test_score': array([0.89812713, 0.86975028, 0.92877412, 0.86975028, 0.93586833,\n",
      "       0.86975028]), 'split3_test_score': array([0.90661368, 0.87737724, 0.93329549, 0.87936418, 0.93897247,\n",
      "       0.87482259]), 'split4_test_score': array([0.90320749, 0.87709339, 0.92790236, 0.87709339, 0.93698552,\n",
      "       0.87709339]), 'split5_test_score': array([0.89951746, 0.87567414, 0.92790236, 0.87567414, 0.94010786,\n",
      "       0.87567414]), 'mean_test_score': array([0.90326853, 0.87682702, 0.93070337, 0.87715813, 0.9381297 ,\n",
      "       0.87640131]), 'std_test_score': array([0.00480381, 0.00398699, 0.00253927, 0.00409985, 0.00151627,\n",
      "       0.00404153]), 'rank_test_score': array([3, 5, 2, 4, 1, 6]), 'split0_train_score': array([0.90406993, 0.87506386, 0.9354033 , 0.87506386, 0.94914004,\n",
      "       0.87506386]), 'split1_train_score': array([0.90565931, 0.87795879, 0.93528978, 0.87795879, 0.94874269,\n",
      "       0.87795879]), 'split2_train_score': array([0.90463757, 0.8777885 , 0.93733326, 0.8777885 , 0.94970767,\n",
      "       0.8777885 ]), 'split3_train_score': array([0.9050403 , 0.87336815, 0.93608809, 0.87631967, 0.9487456 ,\n",
      "       0.87325463]), 'split4_train_score': array([0.90487002, 0.88017936, 0.93654217, 0.88017936, 0.94891588,\n",
      "       0.88017936]), 'split5_train_score': array([0.90577818, 0.87660347, 0.93563401, 0.87660347, 0.9493132 ,\n",
      "       0.87660347]), 'mean_train_score': array([0.90500922, 0.87682702, 0.93604844, 0.87731894, 0.94909418,\n",
      "       0.8768081 ]), 'std_train_score': array([0.00058518, 0.00218197, 0.0007141 , 0.00160248, 0.00034237,\n",
      "       0.00221217])}\n"
     ]
    }
   ],
   "source": [
    "print(svc_vc_fs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9381966670345436"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_vc_fs.score(X_test_small_fs, y_test_small_fs_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "svc_best_fs = svc_vc_fs.best_estimator_\n",
    "joblib.dump(svc_best_fs, os.path.join(os.curdir,'Models', 'model_svc_survive_fs.joblib'))\n",
    "svc_best_loaded_fs = joblib.load(os.path.join(os.curdir,'Models', 'model_svc_survive_fs.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svc_best_loaded.predict([X_test_small.iloc[0, :]])\n",
    "svc_best_loaded_fs.predict([[i for i in range(24)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using models in games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **no fires**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">**enemy frequency = 1 / 2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data with no fires has a ratio of movements vs cost lives to be (5 test runs):\n",
    "\n",
    "|              |    1    |    2    |    3    |    4    |    5    |\n",
    "|--------------|--------:|--------:|--------:|--------:|--------:|\n",
    "|Lives Consumed| 2372.000| 2770.000| 2627.000| 2250.000| 1989.000|\n",
    "|Moves         |12000.000|12000.000|12000.000|12000.000|12000.000|\n",
    "|Ratio         |    0.198|    0.231|    0.219|    0.187|    0.166|\n",
    "\n",
    "After training with the survival data, result becomes (5 test runs):\n",
    "\n",
    "|          |    1    |    2    |    3    |    4    |    5    |\n",
    "|----------|--------:|--------:|--------:|--------:|--------:|\n",
    "|Lives Consumed| 2448.000| 2002.000| 2445.000| 2263.000| 2313.000|\n",
    "|Moves     |12001.000|12001.000|12001.000|12001.000|12001.000|\n",
    "|Ratio     |    0.204|    0.167|    0.204|    0.189|    0.193|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "The improvements are not very obvious, because there is a lot of topedoes and enemies in the game.\n",
    "\n",
    "Next I will try to decrease the amount of topedoes and enemies, to see if there will be higher improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">**enemy frequency = 1 / 8**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data with no fires has a ratio of movements vs cost lives to be (5 test runs):\n",
    "\n",
    "|              |    1    |    2    |    3    |    4    |    5    |\n",
    "|--------------|--------:|--------:|--------:|--------:|--------:|\n",
    "|Lives Consumed|  675.000|  593.000|  578.000|  643.000|  653.000|\n",
    "|Moves         |12000.000|12000.000|12000.000|12000.000|12000.000|\n",
    "|Ratio         |    0.056|    0.049|    0.048|    0.054|    0.054|\n",
    "\n",
    "After training, the ratio becomes (5 test runs):\n",
    "\n",
    "|              |    1    |    2    |    3    |    4    |    5    |\n",
    "|--------------|--------:|--------:|--------:|--------:|--------:|\n",
    "|Lives Consumed|  509.000|  464.000|  449.000|  467.000|  433.000|\n",
    "|Moves         |12001.000|12001.000|12001.000|12001.000|12001.000|\n",
    "|Ratio         |    0.042|    0.039|    0.037|    0.039|    0.036|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvements are obvious, the lives consumed ratio dropped about ~10%. But still not good enough. With fires turned on, we can see if there is more improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ** With fires**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">**enemy frequency = 1 / 2**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data with no fires has a ratio of movements vs cost lives to be (5 test runs):\n",
    "\n",
    "|              |    1    |    2    |    3    |   4    |    5    |\n",
    "|--------------|--------:|--------:|--------:|-------:|--------:|\n",
    "|Lives Consumed|  975.000| 1123.000| 1090.000|  996.00| 1070.000|\n",
    "|Moves         |12000.000|12000.000|12000.000|12000.00|12000.000|\n",
    "|Ratio         |    0.081|    0.094|    0.091|    0.08|    0.089|\n",
    "\n",
    "After training, the ratio becomes (5 test runs):\n",
    "\n",
    "\n",
    "|              |    1    |    2    |    3    |    4    |    5    |\n",
    "|--------------|--------:|--------:|--------:|--------:|--------:|\n",
    "|Lives Consumed|  758.000|  703.000|  677.000|  608.000|  619.000|\n",
    "|Moves         |12001.000|12001.000|12001.000|12001.000|12001.000|\n",
    "|Ratio         |    0.063|    0.059|    0.056|    0.051|    0.052|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The improvement is better. The ratio dropped ~30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">**enemy frequency = 1 / 8**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data with no fires has a ratio of movements vs cost lives to be (5 test runs):\n",
    "\n",
    "|              |    1    |    2    |    3    |    4    |    5    |\n",
    "|--------------|--------:|--------:|--------:|--------:|--------:|\n",
    "|Lives Consumed|  271.000|  259.000|  271.000|  247.000|  287.000|\n",
    "|Moves         |12000.000|12000.000|12000.000|12000.000|12000.000|\n",
    "|Ratio         |    0.023|    0.022|    0.023|    0.021|    0.024|\n",
    "\n",
    "After training, the ratio becomes (5 test runs):\n",
    "\n",
    "|              |    1    |    2    |    3    |    4    |    5    |\n",
    "|--------------|--------:|--------:|--------:|--------:|--------:|\n",
    "|Lives Consumed|  150.000|  151.000|   91.000|  114.000|  144.000|\n",
    "|Moves         |12001.000|12001.000|12001.000|12001.000|12001.000|\n",
    "|Ratio         |    0.012|    0.013|    0.008|    0.009|    0.012|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The improvement is again better. The ratio dropped ~50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
